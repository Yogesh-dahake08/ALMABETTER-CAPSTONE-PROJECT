{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yogesh-dahake08/Regression-Retail-Sales-Prediction/blob/main/Retail_Sales_Prediction_Regression_Assessment_Yogesh_Dahake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdDsZNEGiyLR"
      },
      "source": [
        "# **Project Name**   \n",
        "**Retail Sales Prediction : Predicting Sales of a Major Store Chain Rossmann**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrOtsb09jCVm"
      },
      "source": [
        "##### **Project Type**    - Supervised Regression Machine Learning\n",
        "##### **Contribution**    - Yogesh Dahake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVx7JPkojJQK"
      },
      "source": [
        "# **Project Summary -**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RICltWuMjOMg"
      },
      "source": [
        "**Rossman Sales Prediction** data contains historical sales data for a retail store chain. The data includes information about the store such as Competitior’s Detail, holiday’s, number of the customers, sale transaction's date and amount of sale on each day. We were tasked **to forecast the \"Sales\"** for the test set.\n",
        "\n",
        "After understanding the data and getting variables, we first gathered and **cleaned the data, handled the null values** and finally for getting better results we **merged** two datasets after that we have also typecasted the needed features into required format by **type casting** in order to visualize them properly. We performed indepth **EDA** and plotted different types of graphs by separating them into univariate, bivariate and multivariate categories as a result, We came accross some meaningful insights that helped us to make future decisions of ML model pipeline. Then further on, using **feature engineering** and data preprocessing  we have extracted new features like PromoDuration and CompetitionDuration with the help of some features which are not directly impacting to Sales. We also tried to get some impacting features by removing **multicollinearity** within the independent variables with the help of **various inflation factor(VIF)**. Under the umbrella of feature engineering we have detected and treated the outliers with the help of **IQR technique** and capped all the outliers of continous features in 25-75 percentile. Also, we noticed that some of the features were categorical in nature and ML model can not understand the language of alphabets(strings).So, we have encoded them into numericals using **One-Hot Encoding technique** as they were unordered in nature.\n",
        "\n",
        "In order to get normally distributed data we have applied various **transformation techniques** such as Logarithmic Transformation, Exponential Transformation, Square root Transformation and some others as well and plotted the quantie-quantile plot to visualize how far our data points are from the normal distribution.To scale the data We used the sklearn library **StandardScaler**.\n",
        "\n",
        "Now as we are ready with our final features we **splitted** it into training and testing sets. Next, we choose some **machine learning algorithms** and use the training data to train the model. Finally, we evaluated the model's performance on the testing data to see how well it is able to predict the sales for the real time data. For this task we used many machine learning algorithms, such as **linear regression, decision trees, random forests, LightGBM and XGboost**. For the less complex models like **Linear Regression**, we could achieve the **r2 score of 0.75** and **accuracy i.e 100-MAPE of 93%** even after applying regularisation techniques such as Lasso,Ridge and Elstic Net. In order to capture **more variance** and train our model more aptly, we decided to go for **more complex models** one by one. After training our datasets on decision trees, random forests, LightGBM and XGboost, we could gather the **r2 score of 0.94** with the best **accuracy of 97% using XGboost** with mean absolute percentage error of only 2%. Also we got the mean of residuals as 0.0 which is indicating towards perfectly normally distributed residuals which is one of the good characteristics of good residual plot. From the above experiments and identifications, we have choosen the **XGboost** as our **final optimal model** among all 5 models for deployment as it is predicting the highest accuracy with the least error.\n",
        "\n",
        "Overall, while building a machine learning model on Rossman Sales Prediction Data, we applied combination of **data processing, machine learning techniques, and model evaluation skills.** It was a challenging task and we faced some failures as well but with the right approach and knowledge, we successfully created a model that can accurately predict sales upto six weeks in advance!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Afg8VKWtjXkf"
      },
      "source": [
        "# GitHub Link -https://github.com/Yogesh-dahake08/Regression-Retail-Sales-Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGJQ6yUljsl-"
      },
      "source": [
        "# **Problem Statement**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIe6kQWwjvzc"
      },
      "source": [
        "**Rossmann Stores Data.csv** - historical data including Sales\n",
        "\n",
        "**Store.csv** - supplemental information about the stores\n",
        "\n",
        "**Data fields**\n",
        "\n",
        "Most of the fields are self-explanatory. The following are descriptions for those that aren't.\n",
        "\n",
        "**Id** - an Id that represents a (Store, Date) duple within the test set\n",
        "\n",
        "**Store** - a unique Id for each store\n",
        "\n",
        "**Sales** - the turnover for any given day (this is what you are predicting)\n",
        "\n",
        "**Customers** - the number of customers on a given day\n",
        "\n",
        "**Open** - an indicator for whether the store was open: 0 = closed, 1 = open\n",
        "\n",
        "**StateHoliday** - indicates a state holiday.\n",
        "\n",
        "**SchoolHoliday** - indicates if the (Store, Date) was affected by the closure of public schools\n",
        "\n",
        "**StoreType** - differentiates between 4 different store models: a, b, c, d\n",
        "\n",
        "**Assortment** - describes an assortment level: a = basic, b = extra, c = extended\n",
        "\n",
        "**CompetitionDistance** - distance in meters to the nearest competitor store\n",
        "\n",
        "**CompetitionOpenSince[Month/Year]** - gives the approximate year and month of the time the nearest competitor was opened\n",
        "\n",
        "**Promo** - indicates whether a store is running a promo on that day\n",
        "\n",
        "**Promo2** - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
        "\n",
        "**Promo2Since[Year/Week]** - describes the year and calendar week when the store started participating in Promo2\n",
        "\n",
        "**PromoInterval** - describes the consecutive intervals Promo2 is started, naming the months the promotion is started a new. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzNHayRBncGf"
      },
      "source": [
        "# **General Guidelines** : -  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRbxNE5ij_Vd"
      },
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnlakbHtnU1K"
      },
      "source": [
        "# ***Let's Begin !***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhSKJnCwnjjY"
      },
      "source": [
        "## ***1. Know Your Data***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TbrVfkrnmd1"
      },
      "source": [
        "### Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chyblf7anvmJ"
      },
      "outputs": [],
      "source": [
        "# Importing basic libraries for data processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "# Importing libraries for data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# importing missingo library which helps us to visualize the missing values\n",
        "import missingno as msno\n",
        "\n",
        "# Adding this to ignore future warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Importing pearson corelation library for hypothesis testing\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "#Importing lightGBM and xgboost\n",
        "!pip install lightgbm\n",
        "!pip install xgboost\n",
        "\n",
        "# Importing libraries for model implimentation\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# Importing libraries for hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Importing libraries for visualizing decison tree\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn import tree\n",
        "from IPython.display import SVG\n",
        "from graphviz import Source\n",
        "from IPython.display import display\n",
        "\n",
        "# Importing regression metrics to check the model performance\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "# Importing module to see execution time in mili seconds\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime\n",
        "\n",
        "# Improting library for saving model as pickle file\n",
        "import pickle\n",
        "\n",
        "# Importing SHAP for model explainability\n",
        "!pip install SHAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_HVbBe8n8Ho"
      },
      "source": [
        "### Dataset Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbkSZAbqn_kM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFvb5nP6oOmp"
      },
      "outputs": [],
      "source": [
        "# Load Dataset by mounting drive in colab\n",
        "df_rossmann= pd.read_csv('/content/drive/MyDrive/Rossmann Stores Data.csv')\n",
        "df_store= pd.read_csv('/content/drive/MyDrive/store.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r-me3FNokfX"
      },
      "source": [
        "### Dataset First View"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjrPEJ1Dolvl"
      },
      "outputs": [],
      "source": [
        "# Dataset First Look of rossmann dataset\n",
        "df_rossmann.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yaq_ruUouLv"
      },
      "outputs": [],
      "source": [
        "# Dataset First Look of store dataset\n",
        "df_store.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "perluSd5ozz1"
      },
      "source": [
        "### Dataset Rows & Columns count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFgeHI1ho3k8"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f\"Rossmann dataset: Rows={df_rossmann.shape[0]}, Columns={df_rossmann.shape[1]}\")\n",
        "print(f\"Store dataset: Rows={df_store.shape[0]}, Columns={df_store.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HhENEfgo9-7"
      },
      "source": [
        "### Dataset Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uueZGk8qo--q"
      },
      "outputs": [],
      "source": [
        "# Rossmann Dataset Info\n",
        "df_rossmann.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbnCO2b1pDK_"
      },
      "outputs": [],
      "source": [
        "# Store Dataset Info\n",
        "df_store.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JAj3IFDpKAC"
      },
      "source": [
        "#### Duplicate Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjZ8fLHJpKzj"
      },
      "outputs": [],
      "source": [
        "# Dataset Duplicate Value Count\n",
        "print(f\"Number of duplicated rows in Rossmann dataset: {df_rossmann.duplicated().sum()}\")\n",
        "print(f\"Number of duplicated rows in Store dataset: {df_store.duplicated().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9Kr2l_YpXA9"
      },
      "source": [
        "We do not have any duplicated rows in both the dataset and that is very good for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrJT1E6vpar6"
      },
      "source": [
        "#### Missing Values/Null Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02JRaD0VpYiy"
      },
      "outputs": [],
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(\"-\"*50)\n",
        "print(f\"Null values count in Rossmann dataset:\\n{df_rossmann.isna().sum()}\\n\")\n",
        "print(\"-\"*50)\n",
        "print(f\"Null values count in Store dataset:\\n{df_store.isna().sum()}\")\n",
        "print(\"-\"*50)\n",
        "print(f\"Infinite values count in Rossmann dataset:\\n{df_rossmann.isin([np.inf, -np.inf]).sum()}\\n\")\n",
        "print(\"-\"*50)\n",
        "print(f\"Infinite values count in Store dataset:\\n{df_store.isin([np.inf, -np.inf]).sum()}\")\n",
        "print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxF9kqGmpkI1"
      },
      "source": [
        "We don't have null or infinite values Rossmann's dataset but have some null values in Store dataset and we have to deal with it in future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfUchkGkpk_e"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values in Rossmann dataset\n",
        "msno.bar(df_rossmann,figsize= (8,5), color=\"tab:green\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYBbSbLxpr3d"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values in Store dataset\n",
        "msno.bar(df_store,figsize=(8,5),color=\"tomato\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBy-nC7UpwNu"
      },
      "source": [
        "### What did you know about your dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMk70N-apxoy"
      },
      "source": [
        "Till now we get to know the following points about our dataset:\n",
        "\n",
        "'Rossmann dataset' is having 1017209 rows and 9 columns and does not have any null value.\n",
        "'Stores dataset' is having 1115 rows and 10 columns. It contains null values in total six features viz. CompetitionDistance,CompetitionOpenSinceMonth, CompetitionOpenSinceYear, Promo2SinceWeek,Promo2SinceYear and PromoInterval.\n",
        "There are no duplicate values present in both the datasets.\n",
        "There are total 4 categorical features in 'Rossmann' dataset namely Open, Promo, StateHoliday and SchoolHoliday and 'Stores' contain categorical features namely StoreType, Assortment, Promo2, PromoInterval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeeaURZxpz_y"
      },
      "source": [
        "## ***2. Understanding Your Variables***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U__XJCCRp4Hc"
      },
      "outputs": [],
      "source": [
        "# Dataset Columns\n",
        "print(f\"Column names of Rossmann dataset is:\\n{df_rossmann.columns.tolist()}\")\n",
        "print(f\"Column names of Store dataset is:\\n{df_store.columns.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orBzlUI9p9RN"
      },
      "outputs": [],
      "source": [
        "#Rossmann Dataset Describe\n",
        "df_rossmann.describe(include=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBEUQ_JBqDeD"
      },
      "outputs": [],
      "source": [
        "# Store dataset describe\n",
        "df_store.describe(include=\"all\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sOGTjKLqH66"
      },
      "source": [
        "### Variables Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS6nBNhuqMv5"
      },
      "source": [
        "Rossmann Stores Data.csv - historical data including Sales\n",
        "\n",
        "Store.csv - supplemental information about the stores\n",
        "\n",
        "Data fields\n",
        "\n",
        "Most of the fields are self-explanatory. The following are descriptions for those that aren't.\n",
        "\n",
        "Store - a unique Id for each store\n",
        "\n",
        "Sales - the turnover for any given day (this is what you are predicting)\n",
        "\n",
        "Customers - the number of customers on a given day\n",
        "\n",
        "Open - an indicator for whether the store was open: 0 = closed, 1 = open\n",
        "\n",
        "StateHoliday - indicates a state holiday.\n",
        "\n",
        "SchoolHoliday - indicates if the (Store, Date) was affected by the closure of public schools\n",
        "\n",
        "StoreType - differentiates between 4 different store models: a, b, c, d\n",
        "\n",
        "Assortment - describes an assortment level: a = basic, b = extra, c = extended\n",
        "\n",
        "CompetitionDistance - distance in meters to the nearest competitor store\n",
        "\n",
        "CompetitionOpenSince[Month/Year] - gives the approximate year and month of the time the nearest competitor was opened\n",
        "\n",
        "Promo - indicates whether a store is running a promo on that day\n",
        "\n",
        "Promo2 - Promo2 is a continuing and consecutive promotion for some stores: 0 = store is not participating, 1 = store is participating\n",
        "\n",
        "Promo2Since[Year/Week] - describes the year and calendar week when the store started participating in Promo2\n",
        "\n",
        "PromoInterval - describes the consecutive intervals Promo2 is started, naming the months the promotion is started a new. E.g. \"Feb,May,Aug,Nov\" means each round starts in February, May, August, November of any given year for that store."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtgcP2hsqPDM"
      },
      "source": [
        "### Check Unique Values for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVDwJWURqTIQ"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df_rossmann.columns.tolist():\n",
        "  print(\"The Unique Values of', i, 'are:\", df_rossmann[i].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ddx1wFtyqabw"
      },
      "outputs": [],
      "source": [
        "# Check Unique Values for each variable.\n",
        "for j in df_store.columns.tolist():\n",
        "  print(\"The Unique Values of', j, 'are:\", df_store[j].unique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y12VZJFLqgEP"
      },
      "source": [
        "## 3. ***Data Wrangling***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHHQnsoQqiuF"
      },
      "source": [
        "### Null values treatment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P01GVpjOqjsa"
      },
      "outputs": [],
      "source": [
        "# fetching the observations which contains null values in CompetitionDistance feature\n",
        "df_store[df_store[\"CompetitionDistance\"].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQjh6bwaqnrP"
      },
      "outputs": [],
      "source": [
        "# plotting seaborn box plot to check the outliers in CompetitionDistance features\n",
        "plt.figure(figsize=(15,5))\n",
        "sns.boxplot(x= df_store[\"CompetitionDistance\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPU0JcFiquPf"
      },
      "source": [
        "In order to fill the null values of CompetitionDistance we have 4 options:\n",
        "\n",
        "1. Zero(0) >> Not effective as the corresponding values for CompetitionSinceMonth and CompetitionSinceYear are not zero.\n",
        "2. Mean >> Replacing null values with mean will create blunders as it contains outliers and mean is influenced with outliers.\n",
        "3. Median >> To get good results replacing with median can be a better choice.\n",
        "4. Mode >> Mode can also help to fill the null values in our case.\n",
        "Since, we have two options(Mode and Median) we are going with the median."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdXAVmOTqvpE"
      },
      "outputs": [],
      "source": [
        "# filling null values\n",
        "df_store[\"CompetitionDistance\"].fillna(df_store[\"CompetitionDistance\"].median(), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnnDFofPq3T1"
      },
      "outputs": [],
      "source": [
        "# rechecking if we have any further null values in CompetitionDistance feature\n",
        "df_store[df_store[\"CompetitionDistance\"].isnull()].sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO-bINZzq8I8"
      },
      "outputs": [],
      "source": [
        "# rechecking if we have any null values\n",
        "df_store.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7IBKrKgrA9_"
      },
      "source": [
        "Hurray!! We do not have any further null values in CompetitionDistance feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWqN0sx7rB0d"
      },
      "source": [
        "##### b). Handling null values from ***CompetitionOpenSinceMonth*** and ***CompetitionOpenSinceYear*** feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDO_PSTIrO4P"
      },
      "outputs": [],
      "source": [
        "# fetching the observations which contains null values in CompetitionOpenSinceMonth and CompetitionOpenSinceYear feature\n",
        "df_store[df_store[\"CompetitionOpenSinceMonth\"].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xU_1zzLqrT3X"
      },
      "outputs": [],
      "source": [
        "# plotting seaborn box plot to check the outliers in CompetitionOpenSinceMonth and CompetitionOpenSinceYear features\n",
        "plt.figure(figsize=(15,5))\n",
        "sns.boxplot(df_store[\"CompetitionOpenSinceMonth\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g0qI9cF2rYVd"
      },
      "outputs": [],
      "source": [
        "# plotting seaborn box plot to check the outliers in CompetitionOpenSinceMonth and CompetitionOpenSinceYear features\n",
        "plt.figure(figsize=(15,5))\n",
        "sns.boxplot(df_store[\"CompetitionOpenSinceYear\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPyrbumFrccz"
      },
      "source": [
        "Filling null values of CompetitionOpenSinceMonth and CompetitionOpenSinceYear with Mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKaWxIuErdA4"
      },
      "outputs": [],
      "source": [
        "# filling null values of CompetitionOpenSinceMonth\n",
        "df_store[\"CompetitionOpenSinceMonth\"].fillna(df_store[\"CompetitionOpenSinceMonth\"].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WE4MA3qbrh7u"
      },
      "outputs": [],
      "source": [
        "# filling null values of CompetitionOpenSinceYear\n",
        "df_store[\"CompetitionOpenSinceYear\"].fillna(df_store[\"CompetitionOpenSinceYear\"].mode()[0], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UndKp_F_rlL-"
      },
      "outputs": [],
      "source": [
        "# rechecking if we have any null values in CompetitionOpenSinceMonth and CompetitionOpenSinceYear\n",
        "df_store.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2R9eSRQ2rp2P"
      },
      "source": [
        "##### c). Handling null values from **Promo2SinceWeek, Promo2SinceYear and PromoInterval** feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8mRRw5IruNF"
      },
      "source": [
        "Since corresponding value where Promo2 is 0 for features Promo2SinceWeek, Promo2SinceYear and PromoInterval having null values. So, we are replacing null values with 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ermzj-BDrrcT"
      },
      "outputs": [],
      "source": [
        "# Replacing all the null values of Promo2SinceWeek, Promo2SinceYear and PromoInterval with 0.\n",
        "df_store[\"Promo2SinceWeek\"].fillna(0, inplace=True)\n",
        "df_store[\"Promo2SinceYear\"].fillna(0, inplace=True)\n",
        "df_store[\"PromoInterval\"].fillna(0, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-Q7ubj6r1yJ"
      },
      "outputs": [],
      "source": [
        "# rechecking if our features contains more null values\n",
        "df_store.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Fr9EdaTr6OB"
      },
      "source": [
        "Congratulations we have achieved our first milestone by cleaning up all the null/missing values from both the datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u9uPb5Ir9Sy"
      },
      "source": [
        "#### 2. Merging datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfIRwf-Nr64v"
      },
      "outputs": [],
      "source": [
        "# checking shape of dataset using left join\n",
        "df_left= pd.merge(df_rossmann, df_store, on=\"Store\", how= \"left\")\n",
        "print(df_left.shape)\n",
        "print(f\"Total number of null values obtained from left join: {df_left.isna().sum().sum()}\")\n",
        "\n",
        "# checking shape of dataset using right join\n",
        "df_right= pd.merge(df_rossmann, df_store, on=\"Store\", how= \"right\")\n",
        "print(df_right.shape)\n",
        "print(f\"Total number of null values obtained from right join: {df_right.isna().sum().sum()}\")\n",
        "\n",
        "\n",
        "# checking shape of dataset using inner join\n",
        "df_inner= pd.merge(df_rossmann, df_store, on=\"Store\", how= \"inner\")\n",
        "print(df_inner.shape)\n",
        "print(f\"Total number of null values obtained from inner join: {df_inner.isna().sum().sum()}\")\n",
        "\n",
        "# checking shape of dataset using outer join\n",
        "df_outer= pd.merge(df_rossmann, df_store, on=\"Store\", how= \"outer\")\n",
        "print(df_outer.shape)\n",
        "print(f\"Total number of null values obtained from outer join: {df_outer.isna().sum().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEJw5fW8sESA"
      },
      "source": [
        "Since, we are obtaining the same shape and 0 null values from all the joins therefore we can use any of the join and it won't affact the results.\n",
        "\n",
        "We are following the inner join for our further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds_TED7vsE5x"
      },
      "outputs": [],
      "source": [
        "#using inner join for our further analysis\n",
        "df= pd.merge(df_rossmann, df_store, on=\"Store\", how= \"left\")\n",
        "print(df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCmU9Mm5sKvF"
      },
      "outputs": [],
      "source": [
        "#checking info of our final merged dataset\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajn1v4f6sO_S"
      },
      "source": [
        "#### 3. Typecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s659TKYIsPzA"
      },
      "outputs": [],
      "source": [
        "# Checking dtypes of all the variables of the dataframe\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi1EKt68sWJq"
      },
      "outputs": [],
      "source": [
        "# changing dtype into required format from both the datasets\n",
        "df[\"Date\"]= pd.to_datetime(df[\"Date\"],format=\"%Y/%m/%d\")\n",
        "df[\"CompetitionDistance\"]= df[\"CompetitionDistance\"].astype(int)\n",
        "df[\"CompetitionOpenSinceMonth\"]= df[\"CompetitionOpenSinceMonth\"].astype(int)\n",
        "df[\"CompetitionOpenSinceYear\"]= df[\"CompetitionOpenSinceYear\"].astype(int)\n",
        "df[\"Promo2SinceWeek\"]= df[\"Promo2SinceWeek\"].astype(int)\n",
        "df[\"Promo2SinceYear\"]= df[\"Promo2SinceYear\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWM1FgYQsade"
      },
      "outputs": [],
      "source": [
        "# Verifying the dtypes\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRMIZozDsgJV"
      },
      "source": [
        "### What all manipulations have you done and insights you found?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgSIoydPsjzv"
      },
      "source": [
        "In data wrangling we have divided it into three sections:\n",
        "\n",
        "\n",
        "1.   **Null values treatment**: In this section we have treated all the null values from both the datasets. We did the following replacements:\n",
        "\n",
        "\n",
        "> a. Replaced null values of CompetitionDistance with MEDIAN.\n",
        "\n",
        "> b. Replaced null values of CompetitionOpenSinceMonth and with MODE.\n",
        "\n",
        "> c. Replaced null values of CompetitionOpenSinceYear and with MODE.\n",
        "\n",
        "> d. Replaced null values of Promo2SinceWeek and with 0.\n",
        "\n",
        "> e. Replaced null values of Promo2SinceYear and with 0.\n",
        "\n",
        "> f. Replaced null values of PromoInterval and with 0.\n",
        "\n",
        "\n",
        "2.   **Merging datasets**: We don't want to compromise with quality and quantity of our dataset in order to get the best accuracy in ML model implementation. So, we were wondering to use the best join for the good results and we got to know with our R&D that every join is giving the same shape of our merged dataset with 0 null values. So, we have decided to go with the inner join.\n",
        "\n",
        "3.   **Typecasting**: In typecasting section we have typecasted the following features in order to visualize it properly and can feed them as input of ML model:\n",
        "\n",
        "\n",
        "> a. Typecasted the Date feature to Datetime format.\n",
        "\n",
        "> b. Typecasted the CompetitionDistance, CompetitionOpenSinceMonth, CompetitionOpenSinceYear, Promo2SinceWeek, Promo2SinceYear feature to Integer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOtYgrFfsqnj"
      },
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJxRW2QjswOi"
      },
      "source": [
        "As some stores in the dataset were temporarily closed due to refurbishment and they won't provide us any information , so we will not consider those store which were closed . We will remove those rows where stores were closed ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXayK4wmsfh7"
      },
      "outputs": [],
      "source": [
        "# Considering those rows where stores are open and sales are not zero.\n",
        "df = df[(df['Open']==1) & (df['Sales']!=0)]\n",
        "\n",
        "# Now since every store in our dataset are opened , we don't need 'Open' column and we will drop this column\n",
        "df.drop(['Open'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZElm_7Srs2d-"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tu885CIzs85H"
      },
      "source": [
        "Now we will do data visualization in an structured way following ' UBM ' rule\n",
        "\n",
        "*   Univariate Analysis\n",
        "*   Bivariate Analysis\n",
        "\n",
        "*   Multivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1wu7Scjs90D"
      },
      "source": [
        "###**Univariate Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDRyhk1StEM1"
      },
      "source": [
        "####**Chart-1: Checking frequency distribution of continous features-Sales, CompetitionDistance, Customers:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZRvreR_MP3S"
      },
      "outputs": [],
      "source": [
        "#Checking Frequency distribution for continous features:\n",
        "plt.figure(figsize=(20,10))\n",
        "\n",
        "#First plot(Sales vs Frequency)\n",
        "plt.subplot(2,2,1)\n",
        "plt.xlabel(\"Sales\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "sns.kdeplot(df[\"Sales\"], color=\"Green\", shade = True)  #kernel density estimate (KDE) plot\n",
        "plt.title('Density distribution of Sales',size = 15)\n",
        "\n",
        "#Second plot(CompetitionDistance vs Frequency)\n",
        "plt.subplot(2,2,2)\n",
        "plt.xlabel(\"CompetitionDistance\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "sns.kdeplot(df[\"CompetitionDistance\"], color=\"Blue\", shade = True) #kernel density estimate (KDE) plot\n",
        "plt.title('Density distribution of CompetitionDistance',size = 15)\n",
        "\n",
        "#Third plot(Customers vs Frequency)\n",
        "plt.subplot(2,2,3)\n",
        "plt.xlabel(\"Customers\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "sns.kdeplot(df[\"Customers\"], color=\"Red\", shade = True) #kernel density estimate (KDE) plot\n",
        "plt.title('Density distribution of Customers',size = 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk8fwPDUM5OA"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DCj00r4M-JF"
      },
      "source": [
        "We picked this chart as it shows whether the observations are high or low and also whether they are concentrated in one area or spread out across the entire scale for continous features only."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lru_-ItHNDOI"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVtfO7B7ND7z"
      },
      "source": [
        "1. Avarage Sales value is under 5000 and graph is rightly skewed, which shows most of the stores are open where the middle class resides.\n",
        "\n",
        "2. We can clearly observe that most of the stores have their competition within\n",
        "   5Km range which indicates mostly competiting stores are located nearby to   each other.\n",
        "\n",
        "3. Avarage number of customer visiting stores is 700."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqYhcYeQNG4u"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWJoxBRqNJ_3"
      },
      "source": [
        "1. As we can see most of the sales are under 1000, to expand their business and sales they should **open their stores in posh area** or where the upper class lives.\n",
        "\n",
        "2. We plotted frequency distribution graph for sales, customer and competition distance with the help of which we came to know that many competiting stores are densly located. so, business should **plan different strategies** to sustain in the competitive market."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xRpO8_0NNY3"
      },
      "source": [
        "####**Chart-2: Checking frequency distribution of continous features-SchoolHoliday, Promo, Promo2:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6J3vNWKM55Q"
      },
      "outputs": [],
      "source": [
        "#Lets check distribution for Some discrete features(SchoolHoliday,Promo,Promo2):\n",
        "labels = 'Not SchoolHoliday' , 'SchoolHoliday'\n",
        "sizes = df.SchoolHoliday.value_counts()\n",
        "colors = ['pink', 'yellow']\n",
        "explode = (0.2, 0.0)\n",
        "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
        "        autopct='%1.1f%%', shadow=True)\n",
        "plt.axis('equal')\n",
        "plt.title(\"Percentage of School Holiday\",fontsize=20)\n",
        "plt.legend( labels, loc=\"best\")\n",
        "plt.plot()\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(4,4)\n",
        "plt.show()\n",
        "\n",
        "labels = 'Promo' , 'Not Promo'\n",
        "sizes = df.Promo.value_counts()\n",
        "colors = ['pink', 'yellow']\n",
        "explode = (0.2, 0.0)\n",
        "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
        "        autopct='%1.1f%%', shadow=True)\n",
        "plt.axis('equal')\n",
        "plt.title(\"Percentage of promoted stores\",fontsize=20)\n",
        "plt.legend( labels, loc=\"best\")\n",
        "plt.plot()\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(4,4)\n",
        "plt.show()\n",
        "\n",
        "labels = 'Promo2' , 'Not Promo2'\n",
        "sizes = df.Promo2.value_counts()\n",
        "colors = ['pink', 'yellow']\n",
        "explode = (0.2, 0.0)\n",
        "plt.pie(sizes, explode=explode, labels=labels, colors=colors,\n",
        "        autopct='%1.1f%%', shadow=True)\n",
        "plt.axis('equal')\n",
        "plt.title(\"Percentage of stores who are into continuous promotion\",fontsize=20)\n",
        "plt.legend( labels, loc=\"best\")\n",
        "plt.plot()\n",
        "fig=plt.gcf()\n",
        "fig.set_size_inches(4,4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B37jekDHNc43"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djCR3sviNdjw"
      },
      "source": [
        "We choose the pie chart as it represents the contribution of each part of the data to a whole where the arc size of each slice is directly proportional to the contribution of that part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97WNLhCBNgQb"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW7ju7ZYNkme"
      },
      "source": [
        "1.From first pie chart We can say that market will observe approx 20% of school Holidays.\n",
        "\n",
        "2.We see that 44.6% of stores are into promotions and 55.4% of stores are not into promotions .The reason that more stores are not into promotions may be they don't have the enough budget or they may not see enough need for promotions.\n",
        "\n",
        "3.From Third chart we can infer that there is almost equal percentage of stores getting promoted and the one which are not getting promoted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW3AiqxjNnp3"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KM1nQhwfNqwc"
      },
      "source": [
        "1. From above pie chart we understood that percentage of stores getting promoted is more than non promoting stores but sales are positively correlated to promo meaning **if stores are getting promoted sale is increasing**. So, business should try promoting the stores to increase the revenue.\n",
        "2. Later we will see if their is any impact of School holiday on sale on sale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFNpwN-WNtAx"
      },
      "source": [
        "###**Bivariate Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtXgoevINw4X"
      },
      "source": [
        "#### **Chart-1: Sales vs DayOfWeek**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUO2rKwSN1zx"
      },
      "outputs": [],
      "source": [
        "# Sales Vs DayOfWeek\n",
        "plt.figure(figsize=(9,7))\n",
        "plots=sns.barplot(x=df['DayOfWeek'],y=df['Sales'],edgecolor='black')\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=15, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6WtRIl3NzpA"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYozVSgiN-K1"
      },
      "source": [
        "We used **bar plots** because they are a useful tool for visualizing and understanding **categorical data**, and can be an effective way to communicate information to the wide audience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQMDpqZpOBDq"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9D30-7xmOEkz"
      },
      "source": [
        "Day 1 and day 7 witness the highest sale indicating they are probably days falling on the weekend. Day 2 to day 6 generate medium to low sales indicating they are probably weekdays where customer footfall is low."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gp7waZ2TOJkb"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOytM_Q3ONE6"
      },
      "source": [
        "1. Day 1 and 7 have good number of sales so we can **hire more staff specially for these days** and delivery boys for more revenue.\n",
        "2. Since only 2 days are witnessing good sales, store can **target remaining 5 days in a week to milk more revenue**. It can run exciting offers and attract more customers on weekdays to generate more revenue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wfBi8emORtR"
      },
      "source": [
        "#### **Chart-2: Sales vs Year**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbBJsFk1OUpr"
      },
      "outputs": [],
      "source": [
        "# Sales vs Year\n",
        "plt.figure(figsize=(7,5))\n",
        "plots=sns.barplot(x=df[\"Date\"].dt.year,y=df['Sales'],edgecolor='black')\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=15, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbHx86xvOd-N"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkFtCgGFOxqd"
      },
      "source": [
        "We used this plot to visualize the distribution of sales in 2013, 2014 and 2015. Count plots can help us understand how the values of a variable are distributed within the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fjObZshOzR0"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_A9wiuJO7eG"
      },
      "source": [
        "For any business, year-on-year sales growth is a good parameter to access the store growth. Here we can observe that even though sales are increasing year-on-year, **sales growth is not even 10%**. So the owners need to put extra efforts to increase the yearly sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKKsEqzlO8KS"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1y3DP95PDMF"
      },
      "source": [
        "Yes. The insights from this graph are actually very important for the owners. Since the growth is not at very commendable pace, they need to **target more customers and bring some changes in the operations.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_rcA4WjPIeF"
      },
      "source": [
        "#### **Chart-3: Sales vs Month**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zed64rZ8PP9W"
      },
      "outputs": [],
      "source": [
        "# Month vs sales\n",
        "plt.figure(figsize=(15,5))\n",
        "plots=sns.barplot(x=df['Date'].dt.month,y=df['Sales'],edgecolor='black')\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=15, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        "plt.xlabel(\"Month\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMU5BLy1Pd9B"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO6se8yPPhEa"
      },
      "source": [
        "To know the patterns or trends in the data, such as a **peak in activity or sales during a particular month of the year.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZacrZ1KZPkfD"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30csgqfpPlOc"
      },
      "source": [
        "The countplot highlights that **December being a festive month** attracts more sale than the rest of the months. Also, November has slightly more sales than other months. This could be due to the **'Black Friday'** sale which is very popular across the globe. As Rossmann Stores deals in health and beauty products, it can be guessed that November and December sales are due to the celebratory nature of people who love to buy beauty/health products leading to the sudden increase in sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxpY0YUfPryK"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DELD-TtnPxF3"
      },
      "source": [
        "Yes, definitely these insights create a positive business impact as business owners will try to **keep more goods to cater with the business needs** and also they can increase the revenue by keeping **stores open even on weekends or holidays** as customers are aiming to buy more in this period of month."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YhGdVOCPz3b"
      },
      "source": [
        "#### **Chart-4: Sales vs Customer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ycIwlp9OQCj9"
      },
      "outputs": [],
      "source": [
        "# Sale Vs Customer\n",
        "plt.figure(figsize=(7, 7))\n",
        "sns.scatterplot(x=df['Customers'], y=df['Sales'])\n",
        "\n",
        "plt.xlabel('Customers')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales vs Customers')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfQ5QHgQQNZE"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvdC6FGSQPwr"
      },
      "source": [
        "We chose scatter plot because we wanted to plot the relationship between the number of customers visiting the store and the total sales.They are useful for identifying outliers in the data, as well as to determine the correlation between two variables,and to verify the linear trend of our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDlliIcQQT_c"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgPGsSRhQU5p"
      },
      "source": [
        "This above scatterplot show a **positive correlation between 'Sales' and 'Customers'**. As the number of customers increases, the sales also tend to increase. We understood from the above trend that our data shows linear trend between these two variables - customers and sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu3gerldQYcW"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFKYNIWQQbGI"
      },
      "source": [
        "Customers and sales are directly proportional to each other and are highly correlated. It can help businesses to promote their strategies using **marketing campaigns, advertisements** to attract more customers ultimately shooting up the sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWvgnWE4QdtF"
      },
      "source": [
        "#### **Chart-5: Sales vs Promo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSpgFvN8QmEu"
      },
      "outputs": [],
      "source": [
        "# Sales vs Promo\n",
        "plt.figure(figsize=(7,7))\n",
        "plots=sns.barplot(x=df['Promo'],y=df['Sales'],edgecolor='black')\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=15, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ5OZIjzQtoG"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF7aR9U9QuNp"
      },
      "source": [
        "To know the count of how many sales were made at stores that were running promotions and how many were made at stores that were not running promotions. We used bar plots because they are a useful tool for visualizing and understanding categorical data, and can be an effective way to communicate information to the wide audience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYH9CZUEQx-Z"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1KPvGHSQ05C"
      },
      "source": [
        "From the above plot we see the **effectiveness of promotions on increasing sales**. So we can infer that as the stores are getting promoted, sales are increasing on large basis showing positive correlation between promo and sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQl27sSsQ3Dr"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWDLaLllQ5il"
      },
      "source": [
        "Business owners should try to **promote the stores** to sustain in the market eventually resulting increment in the sales amount."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5EYhfPjQ882"
      },
      "source": [
        "#### **Chart-6: Sales vs StateHoliday**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URuzv6fdRA_z"
      },
      "outputs": [],
      "source": [
        "# Sales vs StateHoliday\n",
        "plt.figure(figsize=(7,7))\n",
        "plots=sns.barplot(x=df['StateHoliday'],y=df['Sales'],edgecolor='black')\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=15, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M8mcRywphqQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8agQvks0phqQ"
      },
      "source": [
        "To know the number of sales for various State Holidays ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgIPom80phqQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp13pnNzphqQ"
      },
      "source": [
        "From the above plot, we observe that sales are highest for state holiday 'B' followed by state holiday 'C'. One interesting insight here is **sales are comparitively low during normal days**(i.e with no state holiday).The factors that contribute to higher sales on these holidays could be increased consumer spending, special promotions or events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMzcOPDDphqR"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4Ka1PC2phqR"
      },
      "source": [
        "The business can create special offers or bundle products to increase the sales during the religious festivals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-EpHcCOp1ci"
      },
      "source": [
        "#### **Chart-7: Sales vs SchoolHoliday**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "outputs": [],
      "source": [
        "# Sales vs SchoolHoliday\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "plots=sns.barplot(x=df['SchoolHoliday'],y=df['Sales'],edgecolor='black')\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=15, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_VqEhTip1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vsMzt_np1ck"
      },
      "source": [
        "To know the count of how many sales were made at stores on school holiday and on non-school holiday ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGJKyg5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      },
      "source": [
        "We can see there is **not much difference in sales**. However,sales is more on school holidays . It is possible that school holidays are more likely to be associated with families going on vacation or parents taking time off work to spend with their children, which could lead to increase in consumer spendings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "druuKYZpp1ck"
      },
      "source": [
        "As we can see from the graph, it is not making much difference whether there is school holiday or not. Still, businesses can target school holidays and run more promotional offers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3dbpmDWp1ck"
      },
      "source": [
        "#### **Chart-8: Sales vs StoreType**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gg48yksMSssc"
      },
      "outputs": [],
      "source": [
        "# Sales vs StoreType\n",
        "plt.figure(figsize=(7,7))\n",
        "plots=sns.barplot(x=df['StoreType'],y=df['Sales'],edgecolor='black')\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=15, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylSl6qgtp1ck"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2xqNkiQp1ck"
      },
      "source": [
        "To know which kind of store is able to generate maximum profit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWILFDl5p1ck"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-lUsV2mp1ck"
      },
      "source": [
        "1. Sales for the store type b is the highest . **Store type B might be located in a more affluent or high-traffic area**, which would increase the number of potential customers. Store type B may have a more favorable layout, which makes it more attractive to customers and makes it easier for them to find the products they want, resulting in more sales.\n",
        "\n",
        "2. Earlier we have drawn the univeriate graph of Store type to know which type of store are more in numbers, and we found that store type a,c,d are more but this is not the case with the sales even though **storetype_b are less still they are making more profit**.\n",
        "\n",
        "3. It might be the case that store type_b contain **costly**, **luxerious items**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7G43BXep1ck"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wwDJXsLp1cl"
      },
      "source": [
        "1. Since store type 'a','c' and 'd' are generating similar sales and lower than store type 'b', they can follow the business strategies, **marketing tactics of store type 'b'.**\n",
        "\n",
        "2. Company should **open stores of type_b** to gain more profit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ag9LCva-p1cl"
      },
      "source": [
        "#### **Chart-9: Sales vs Assortment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0Em10XnTJ2F"
      },
      "outputs": [],
      "source": [
        "# Sales vs Assortment\n",
        "plt.figure(figsize=(7,7))\n",
        "plots=sns.barplot(x=df['Assortment'],y=df['Sales'],edgecolor='black')\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=15, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6MkPsBcp1cl"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V22bRsFWp1cl"
      },
      "source": [
        "To know the Sales for various Assortments or which assortment is beneficial for more profit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cELzS2fp1cl"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      },
      "source": [
        "1. Earlier we have seen that **assortment_B are less in numbers compared to a,b,c** but with the help of Bivariate graph we can see that assortment_B caters the maximum amount of sales.\n",
        "\n",
        "2. Sales are highest for the assortment b . This assortment may have a good mix of **products that are in high demand or that are unique to the store**, which would result in more sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MPXvC8up1cl"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GL8l1tdLp1cl"
      },
      "source": [
        "Since sales are highest for assortment 'b', Rossman owners can **target more on this specific combination** and reduce the dependency on assortment 'a' and assortment 'c'. **This could surely be a cost effective move**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC_X3p0fY2L0"
      },
      "source": [
        "#### **Chart-11: Sales vs CompetitionOpenSinceYear**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKXaPsEdTiPC"
      },
      "outputs": [],
      "source": [
        "# Sale Vs CompetitionOpenSinceYear\n",
        "plt.figure(figsize=(15,6))\n",
        "sns.pointplot(x= 'CompetitionOpenSinceYear', y= 'Sales', data=df)\n",
        "plt.title('Plot between Sales and Competition Open Since year')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      },
      "source": [
        "To know the average sales for each year since a competitor opened near the store ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfSqtnDqZNRR"
      },
      "source": [
        "From the Plot we can tell that Sales are high during the year 1900, as there are very few store were operated of Rossmann so there is less competition and sales are high. But as year pass on number of stores increased that means competition also increased and this leads to decline in the sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iK8moSM41Ut"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvpzQeIE4-nW"
      },
      "source": [
        "Since almost every subsequent year reports sudden drop/rise in the sales, owners need to work on their stockings and marketing tactics. The basic reason for this sudden change could be explored more deeply and a plausible solution to it can be reached."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q29F0dvdveiT"
      },
      "source": [
        "#### **Chart-12: Sales vs Promo2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIUccnjfURI6"
      },
      "outputs": [],
      "source": [
        "# Sales vs Promo2\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "plots=sns.barplot(x=df['Promo2'],y=df['Sales'],edgecolor='black')\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=15, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXh0U9oCveiU"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMmPjTByveiU"
      },
      "source": [
        "To know the count of sales with presence and absence of promo2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22aHeOlLveiV"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPQ8RGwHveiV"
      },
      "source": [
        "The barplot shows that customers are slightly less responsive to the stores(i.e sales) that are running consecutive promotions. One possibility could be customers might have already taken advantage of a similar promotion earlier. Another reason could be store might not have invested enough in promoting the promotion to customers, resulting in lower awareness and fewer sales. Also, if the store is running same promotion again and again, it could have resulted into lower customer footfal and ultimately leadind to fewer sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FktfX6vw5cu2"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BkX0dNd5hcd"
      },
      "source": [
        "Yes. The insights are indeed helpful as they are highlighting one of the most important thing about consecutive promotions. Clearly, such promotions are not enough to convert customers into buyers. The store needs to come up with more innovative and more rewardful solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsHYwLY85p7e"
      },
      "source": [
        "#### **Chart-13: Sales vs Promo2SinceYear**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9yDR6TDUvG3"
      },
      "outputs": [],
      "source": [
        "# Sales vs Promo2SinceYear\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "\n",
        "plots=sns.barplot(x=df['Promo2SinceYear'],y=df['Sales'],edgecolor='black')\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=15, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNfFdUmu55G1"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51VXoFAo59EG"
      },
      "source": [
        "To know the sale count for various years since the promo2 started."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wU6JRcR6Cu8"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzkAZhqC6NR_"
      },
      "source": [
        "This barplot explains that sales were still the highest when the store wasn't running any consecutive promotional events. But in 2014, the sales were really shoot up and they are recorded as 2nd highest. Good quality products, better deals, shutdown of competitions etc could be the reasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzEBW87D6SC8"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbcA3yQ16XFr"
      },
      "source": [
        "Yes. The insights are indeed helpful as they are highlighting one of the most important thing about consecutive promotions. Clearly, such promotions are not enough to convert customers into buyers. The store needs to come up with more innovative and more rewardful solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGfu55a36cLJ"
      },
      "source": [
        "#### **Chart-14:  Sales vs PromoInterval**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTgfTBQs6fm2"
      },
      "outputs": [],
      "source": [
        "# Sales vs PromoInterval\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "plots=sns.barplot(x=df['PromoInterval'],y=df['Sales'],edgecolor='black')\n",
        "for bar in plots.patches:\n",
        "    plots.annotate(format(bar.get_height(), '.2f'),\n",
        "                   (bar.get_x() + bar.get_width() / 2,\n",
        "                    bar.get_height()), ha='center', va='center',\n",
        "                   size=15, xytext=(0, 8),\n",
        "                   textcoords='offset points')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCxFfg636l7M"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xb2b1mn6qBq"
      },
      "source": [
        "To know the count of sale for various promo interval ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_r6YI--q6wg8"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iDj9imi63BO"
      },
      "source": [
        "This barplot explains that sales were still the highest when the store wasn't running any consecutive promotional events. Here, we can see the promo interval Jan, Apr, Jul, Oct records the 2nd highest sales as it marks the festive season. However, the other intervals are recording sales that are close to the 1st interval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZOxXSM36738"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_V96HTL7Akb"
      },
      "source": [
        "Business needs to design the pattern of the promotion as sales are decreasing gradually from begining with first quarter of promo interval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET8BllbQ7Hf0"
      },
      "source": [
        "### **Multivariate Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FviAAxxc7PpM"
      },
      "source": [
        "#### **Chart-1 Pair Plot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s__uIuH-Y_Nb"
      },
      "outputs": [],
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n762Q60b7ZEy"
      },
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iou1hEDN7det"
      },
      "source": [
        "It can give multiple visual aids in a single frame and various insights related to the data can be gained in one single look."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz_j1g3O7rzy"
      },
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ4l6W9T7v4B"
      },
      "source": [
        "Pairplot helped us to visualize the relationship between sales and other variables, such as customers, Promotions, competition, and school holidays Columns. This helped us to identify which variables might be useful for predicting sales and inform the design for our machine learning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO90FvsZ72XZ"
      },
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ndmmId176i4"
      },
      "source": [
        "Yes.By plotting pairplt we got to know which features are impacting more on sales aiming for maximum sales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It1csqI58J1D"
      },
      "source": [
        "#### **Chart-2 Correlation Heatmap**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdcTSxEzZW4A"
      },
      "outputs": [],
      "source": [
        "#heatmap\n",
        "plt.figure(figsize = (12,8))\n",
        "sns.heatmap(df.corr(),annot= True,cmap=sns.color_palette('rainbow'),square=True)\n",
        "plt.title('Correlation Heatmap for Playstore data and User review data', size=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-ATYxFrGrvw"
      },
      "source": [
        "## ***5. Hypothesis Testing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      },
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7MS06SUHkB-"
      },
      "source": [
        "1- Null Hypothesis - There is no relation between Customers and Sales\n",
        "\n",
        "Alternate Hypothesis - There is a relationship between Customers and sales\n",
        "\n",
        "2- Null Hypothesis - There is no relation between DayOfWeek and Sales\n",
        "\n",
        "Alternate Hypothesis - There is a relation between DayOfWeek and Sales\n",
        "\n",
        "3- Null Hypothesis - There is no relation between SchoolHoliday and Sales\n",
        "\n",
        "Alternate Hypothesis - There is a relation between SchoolHoliday and Sales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yEUt7NnHlrM"
      },
      "source": [
        "### Hypothetical Statement - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI9ZP0laH0D-"
      },
      "source": [
        "Null Hypothesis - There is no relation between Customers and Sales\n",
        "\n",
        "Alternate Hypothesis - There is a relationship between Customers and sales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I79__PHVH19G"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vnEPGTYaDhd"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "first_sample = df[\"Customers\"].head(60)\n",
        "second_sample = df[\"Sales\"].head(60)\n",
        "\n",
        "stat, p = pearsonr(first_sample, second_sample)\n",
        "print('stat=%.3f, p = %.2f'%(stat, p))\n",
        "if p> 0.05:\n",
        "  print('Accept Null Hypothesis')\n",
        "else:\n",
        "  print('Rejected Null Hypothesis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou-I18pAyIpj"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2U0kk00ygSB"
      },
      "source": [
        "We have used Pearson Correlation test to obtain P-Value along with Pearson Correlation coefficient value.It is a measure of linear correlation between two sets of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF3858GYyt-u"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HO4K0gP5y3B4"
      },
      "source": [
        "We want to check the relationship between two features if they are positively or negatively correlated.P-value and Pearson Correlation coefficient will always have a value between -1 and 1.Here we can see that after applying test on Customers and sales features we got Correlation coefficient as 0.939 which implies that theses two features are having strong positive correlation between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_0_7-oCpUZd"
      },
      "source": [
        "### Hypothetical Statement - 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwyV_J3ipUZe"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      },
      "source": [
        "Null Hypothesis - There is no relation between DayOfWeek and Sales\n",
        "\n",
        "Alternate Hypothesis - There is a relationship between DayOfWeek and sales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yB-zSqbpUZe"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaqD9nM0ahe8"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "first_sample = df[\"DayOfWeek\"].head(60)\n",
        "second_sample = df[\"Sales\"].head(60)\n",
        "\n",
        "stat, p = pearsonr(first_sample, second_sample)\n",
        "print('stat=%.3f, p = %.2f'%(stat, p))\n",
        "if p> 0.05:\n",
        "  print('Accept Null Hypothesis')\n",
        "else:\n",
        "  print('Rejected Null Hypothesis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUvejAfpUZe"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLDrPz7HpUZf"
      },
      "source": [
        "We have used Pearson Correlation test to obtain P-Value along with Pearson Correlation coefficient value.It is a measure of linear correlation between two sets of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd15vwWVpUZf"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xOGYyiBpUZf"
      },
      "source": [
        "We want to check the relationship between two features if they are positively or negatively correlated.P-value and Pearson Correlation coefficient will always have a value between -1 and 1.Here we can see that after applying test on DayOfWeek and sales features we got Correlation coefficient as -0.221 which implies that theses two features are having weak negative correlation between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn_IUdTipZyH"
      },
      "source": [
        "### Hypothetical Statement - 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49K5P_iCpZyH"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gWI5rT9pZyH"
      },
      "source": [
        "Null Hypothesis - There is no relation between SchoolHoliday and Sales\n",
        "\n",
        "Alternate Hypothesis - There is a relationship between SchoolHoliday and sales"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nff-vKELpZyI"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fG6v6xdUa65i"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import pearsonr\n",
        "first_sample = df[\"SchoolHoliday\"].head(60)\n",
        "second_sample = df[\"Sales\"].head(60)\n",
        "\n",
        "stat, p = pearsonr(first_sample, second_sample)\n",
        "print('stat=%.3f, p = %.2f'%(stat, p))\n",
        "if p> 0.05:\n",
        "  print('Accept Null Hypothesis')\n",
        "else:\n",
        "  print('Rejected Null Hypothesis')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLW572S8pZyI"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytWJ8v15pZyI"
      },
      "source": [
        "We have used Pearson Correlation test to obtain P-Value along with Pearson Correlation coefficient value.It is a measure of linear correlation between two sets of data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWbDXHzopZyI"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M99G98V6pZyI"
      },
      "source": [
        "We want to check the relationship between two features if they are positively or negatively correlated.P-value and Pearson Correlation coefficient will always have a value between -1 and 1.Here we can see that after applying test on Customers and sales features we got Correlation coefficient as 0.334 which implies that theses two features are having weak positive correlation between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLjJCtPM0KBk"
      },
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiyOF9F70UgQ"
      },
      "source": [
        "### 1. Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-Ey0OZbbNjE"
      },
      "outputs": [],
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCH1adVd_Ed8"
      },
      "source": [
        "As we have already treated null values so we do not have any more missing/null/duplicate values in our dataset and our dataset is good to go."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wuGOrhz0itI"
      },
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ixusLtI0pqI"
      },
      "source": [
        "We have checked the outliers by plotting the box plot and then replaced the null values of various variables with mean, median,mode and 0 accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1riN9m0vUs"
      },
      "source": [
        "### 2. Handling Outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDL3VkXjbdrq"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU8RJ7D_bh7e"
      },
      "outputs": [],
      "source": [
        "# assigning continous variable features in new variables so that it makes sense while visulatizing through box plots\n",
        "continous_value_feature= [\"DayOfWeek\", \"Sales\", \"Customers\", \"CompetitionDistance\", \"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\", \"Promo2SinceWeek\", \"Promo2SinceYear\"]\n",
        "numeric_features= ['Store', 'DayOfWeek', 'Sales', 'Customers', 'Open', 'Promo', 'StateHoliday', 'SchoolHoliday', 'CompetitionDistance', 'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek','Promo2SinceYear']\n",
        "categorical_features= [\"Date\", \"StoreType\", \"Assortment\", \"PromoInterval\"]\n",
        "print(\"Numeric_features: \",numeric_features)\n",
        "print(\"Categorical_features: \",categorical_features)\n",
        "print(\"Continous_value_feature: \",continous_value_feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRjWdSdPbnBv"
      },
      "outputs": [],
      "source": [
        "# checking outliers with the help of box plot for continous features\n",
        "plt.figure(figsize=(30,15))\n",
        "for n,column in enumerate(continous_value_feature):\n",
        "  plt.subplot(5, 4, n+1)\n",
        "  sns.boxplot(df[column])\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm_UE6hq_d4-"
      },
      "source": [
        "From the above box plots we can see clearly the features \"Sales\", \"Customers\", \"CompetitionDistance\", \"CompetitionOpenSinceMonth\" and \"CompetitionOpenSinceYear\" contains several outliers and rest of the features are fine as they are categorical in nature.\n",
        "\n",
        "Let's define a code to detect the number of outliers and percentage of outliers present in each of the feature in order to handle them accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27MNM-cMbt8p"
      },
      "outputs": [],
      "source": [
        "# defining the code for outlier detection and percentage using IQR.\n",
        "def detect_outliers(data):\n",
        "    outliers = []\n",
        "    data = sorted(data)\n",
        "    q1 = np.percentile(data, 25)\n",
        "    q2 = np.percentile(data, 50)\n",
        "    q3 = np.percentile(data, 75)\n",
        "    print(f\"q1:{q1}, q2:{q2}, q3:{q3}\")\n",
        "\n",
        "    IQR = q3-q1\n",
        "    lwr_bound = q1-(1.5*IQR)\n",
        "    upr_bound = q3+(1.5*IQR)\n",
        "    print(f\"Lower bound: {lwr_bound}, Upper bound: {upr_bound}, IQR: {IQR}\")\n",
        "\n",
        "    for i in data:\n",
        "        if (i<lwr_bound or i>upr_bound):\n",
        "            outliers.append(i)\n",
        "    len_outliers= len(outliers)\n",
        "    print(f\"Total number of outliers are: {len_outliers}\")\n",
        "\n",
        "    print(f\"Total percentage of outlier is: {round(len_outliers*100/len(data),2)} %\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMvkBVGGbxx2"
      },
      "outputs": [],
      "source": [
        "# Determining IQR, Lower and Upper bound and number out outliers present in each of the continous numerical feature\n",
        "for feature in continous_value_feature:\n",
        "  print(feature,\":\")\n",
        "  detect_outliers(df[feature])\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe2pkJEi_qr2"
      },
      "source": [
        "Below mentioned continous features with the percentage of outliers:\n",
        "\n",
        "1. \"Sales\" - 2.62%\n",
        "2. \"Customers\" - 3.75%\n",
        "3. \"CompetitionDistance\" - 9.75%\n",
        "4. \"CompetitionOpenSinceMonth\" - 1.22%\n",
        "5. \"CompetitionOpenSinceYear\" - 2.71%\n",
        "\n",
        "Let's define a function for the outlier treatment using IQR technique and cap the outliers in 25-75 percentile."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mOTFOD7b512"
      },
      "outputs": [],
      "source": [
        "# Defining the function that treats outliers with the IQR technique\n",
        "def treat_outliers_iqr(data):\n",
        "    # Calculate the first and third quartiles\n",
        "    q1, q3 = np.percentile(data, [25, 75])\n",
        "\n",
        "    # Calculate the interquartile range (IQR)\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    # Identify the outliers\n",
        "    lower_bound = q1 - (1.5 * iqr)\n",
        "    upper_bound = q3 + (1.5 * iqr)\n",
        "    outliers = [x for x in data if x < lower_bound or x > upper_bound]\n",
        "\n",
        "    # Treat the outliers (e.g., replace with the nearest quartile value)\n",
        "    treated_data = [q1 if x < lower_bound else q3 if x > upper_bound else x for x in data]\n",
        "    treated_data_int = [int(absolute) for absolute in treated_data]\n",
        "\n",
        "    return treated_data_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64lx8GS0b9as"
      },
      "outputs": [],
      "source": [
        "#Passing all the feature one by one from the list of continous_value_feature in our above defined function for outlier treatment\n",
        "for feature in continous_value_feature:\n",
        "  df[feature]= treat_outliers_iqr(df[feature])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQ3l_nt2cANg"
      },
      "outputs": [],
      "source": [
        "#Replotting the box plots and rechecking the percentage of outliers still available(if any) in the list of continous_value_feature.\n",
        "plt.figure(figsize=(30,15))\n",
        "for n,column in enumerate(continous_value_feature):\n",
        "  plt.subplot(5, 4, n+1)\n",
        "  sns.boxplot(df[column])\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxoRknpacDBZ"
      },
      "outputs": [],
      "source": [
        "# Rechecking the total number of outliers and its percentage present in our dataset.\n",
        "for feature in continous_value_feature:\n",
        "  print(feature,\":\")\n",
        "  detect_outliers(df[feature])\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "578E2V7j08f6"
      },
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGZz5OrT1HH-"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89xtkJwZ18nB"
      },
      "source": [
        "### 3. Categorical Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KAgXJnTcRHd"
      },
      "outputs": [],
      "source": [
        "# checking the features having dtype as object\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCCMh-d1AduC"
      },
      "source": [
        "Clearly we have \"StoreType\", \"Assortment\", \"PromoInterval\" as \"object\". To feed them as an input of our Machine Learning algorithm, we need to use some encoding technique to make dtype of these column as \"integer\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVgBfFQWcWCL"
      },
      "outputs": [],
      "source": [
        "# creating the variable that contains list of \"object\" dtypes\n",
        "obj= [\"StateHoliday\", \"StoreType\", \"Assortment\", \"PromoInterval\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCcwSPlfcYfa"
      },
      "outputs": [],
      "source": [
        "# checking the unique counts of object dype column which is essential to determine the type of encoding to use in various column\n",
        "for unique in obj:\n",
        "  print(f\"{unique}: \")\n",
        "  print(f\"The unique values are: {df[unique].unique()}\")\n",
        "  print(f\"Total number of unique values are: {df[unique].nunique()}\")\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLOs4IrXAoPg"
      },
      "source": [
        "From the above output we can see that the feature \"StateHoliday\" contains \"0\" as string and 0 as int at various observations. So let's convert \"0\"(str) to 0(int)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXOBwPb9cfWk"
      },
      "outputs": [],
      "source": [
        "# replacing \"0\" to 0 and a=b=c=1 for our simplicity as they resembles that there is holiday\n",
        "df[\"StateHoliday\"].replace({\"0\":0, \"a\":1, \"b\":1, \"c\":1}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAuHcCq6cjZ5"
      },
      "outputs": [],
      "source": [
        "# rechecking the unique counts of object dype column which is essential to determine the type of encoding to be use in various column\n",
        "for unique in obj:\n",
        "  print(f\"{unique}: \")\n",
        "  print(f\"The unique values are: {df[unique].unique()}\")\n",
        "  print(f\"Total number of unique values are: {df[unique].nunique()}\")\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H22LWKovAzTM"
      },
      "source": [
        "Since we have several encoding techniques but the major ones are:\n",
        "1. Ordinal encoding: Used when the features are ordinal in nature and have some rank between them.\n",
        "2. Nominal encoding: Used when the features have equal weightage and are nominal in nature.\n",
        "\n",
        "As our all the categorical columns are nominal in nature(do not have any rank or order) so will use One-Hot Encoding (Type of Nominal encoding) in our senario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZozsmWVucoRe"
      },
      "outputs": [],
      "source": [
        "#Lets create a copy of dataframe to avoid blunders with our original dataframe\n",
        "df_new=df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-gadT9gcr6e"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns by dropping the first category\n",
        "# df_new= pd.get_dummies(df, dtype=int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MR2UtMOqBDRI"
      },
      "source": [
        "We can use the above code to create binary dummy variable using ONE-HOT ENCODING for each of the feature but we will face the issue of \"multicollinearity\" or \"dummy variable trap\" as the information given by the one feature can be explained by the other features and this results in the high \"VIF\". So its better to drop the redundant feature (one category among all other category) here only.\n",
        "\n",
        "We can do this easily by passing the argument \"drop_first = True\" in get_dummies without doing it manually, thanks to python code development team to make our tasks easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EllUu7Ccc06E"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns by dropping the first category\n",
        "df_new= pd.get_dummies(df, dtype=int, drop_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33Cf8XNlc24U"
      },
      "outputs": [],
      "source": [
        "# code to see all the features\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ia3U8T1rc4w2"
      },
      "outputs": [],
      "source": [
        "# Let's see first five observations of our dataset\n",
        "df_new.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcf4ug_Gc8A2"
      },
      "outputs": [],
      "source": [
        "# Verifying the dtype\n",
        "df_new.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQN5KX2AMe"
      },
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDaue5h32n_G"
      },
      "source": [
        "a. We have used one-hot encoding technique to change our categorical features of object type into int type by creating their dummies so that it becomes compatible to feed it into various ML algorithms in future.\n",
        "\n",
        "b. Since, we have 3 to 4 unique orderless categories in all the categorical features (which is less in number). So, it is good to use Nominal encoding technique than ordinal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iwf50b-R2tYG"
      },
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMQiZwjn3iu7"
      },
      "source": [
        "#### 1. Expand Contraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "outputs": [],
      "source": [
        "# Expand Contraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVIkgGqN3qsr"
      },
      "source": [
        "#### 2. Lower Casing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "outputs": [],
      "source": [
        "# Lower Casing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkPnILGE3zoT"
      },
      "source": [
        "#### 3. Removing Punctuations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "outputs": [],
      "source": [
        "# Remove Punctuations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hlsf0x5436Go"
      },
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "outputs": [],
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mT9DMSJo4nBL"
      },
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "outputs": [],
      "source": [
        "# Remove White spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49ITxTc407N"
      },
      "source": [
        "#### 6. Rephrase Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "outputs": [],
      "source": [
        "# Rephrase Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeJFEK0N496M"
      },
      "source": [
        "#### 7. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "outputs": [],
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ExmJH0g5HBk"
      },
      "source": [
        "#### 8. Text Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "outputs": [],
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJNqERVU536h"
      },
      "source": [
        "##### Which text normalization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5UmGsbsOxih"
      },
      "source": [
        "#### 9. Part of speech tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "outputs": [],
      "source": [
        "# POS Taging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      },
      "source": [
        "#### 10. Text Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "outputs": [],
      "source": [
        "# Vectorizing Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBMux9mC6MCf"
      },
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      },
      "source": [
        "### 4. Feature Manipulation & Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C74aWNz2AliB"
      },
      "source": [
        "#### 1. Feature Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPKU1vJteR5s"
      },
      "outputs": [],
      "source": [
        "# Manipulate Features to minimize feature correlation and create new\n",
        "# Checking the first five observation of the dataset we have to deal with.\n",
        "df_new.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKtNJl0-eUwL"
      },
      "outputs": [],
      "source": [
        "# Extracting date, month and year from Date feature\n",
        "df_new[\"Day\"]= df_new[\"Date\"].dt.day\n",
        "df_new[\"Month\"]= df_new[\"Date\"].dt.month\n",
        "df_new[\"Year\"]= df_new[\"Date\"].dt.year\n",
        "df_new[\"Week\"]= df_new[\"Date\"].dt.week"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tH6TZ8F4eXnL"
      },
      "outputs": [],
      "source": [
        "#checking first 5 observations\n",
        "df_new.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuU8N-ZVeaO2"
      },
      "outputs": [],
      "source": [
        "# deriving 2 new features:\n",
        "# 1. \"CompetitionDuration -> Duration from which two stores are competiting\"\n",
        "# 2. \"PromoDuration -> Duration from which the store is involved in promotion\"\n",
        "##    Final values are in Months.\n",
        "df_new[\"CompetitionDuration\"]= (df_new[\"Year\"]-df_new[\"CompetitionOpenSinceYear\"])*12 + (df_new[\"Month\"]-df_new[\"CompetitionOpenSinceMonth\"])\n",
        "df_new[\"CompetitionDuration\"] = df_new[\"CompetitionDuration\"].map(lambda x: int(0) if x < 0 else int(x)).fillna(0)\n",
        "\n",
        "# Creating PromoDuration feature\n",
        "df_new[\"PromoDuration\"]= (df_new[\"Year\"]-df_new[\"Promo2SinceYear\"])*12 + (df_new[\"Week\"]-df_new[\"Promo2SinceWeek\"])*7/30.5\n",
        "df_new[\"PromoDuration\"] = df_new[\"PromoDuration\"].map(lambda x: int(0) if x < 0 else int(x)).fillna(0) * df[\"Promo2\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3H542E_3eeZa"
      },
      "outputs": [],
      "source": [
        "# checking first 5 observations after feature extraction\n",
        "df_new.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DejudWSA-a0"
      },
      "source": [
        "#### 2. Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QU_rRGezejSE"
      },
      "outputs": [],
      "source": [
        "# Let's see how sales and other features are related\n",
        "for col in df_new.describe().columns.tolist():\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    feature = df_new[col]\n",
        "    label = df_new['Sales']\n",
        "    correlation = feature.corr(label)\n",
        "    sns.scatterplot(x=feature, y=label, color=\"gray\")\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Sales')\n",
        "    ax.set_title('Sales vs ' + col + '- correlation: ' + str(correlation))\n",
        "    z = np.polyfit(df_new[col], df_new['Sales'], 1)\n",
        "    y_hat = np.poly1d(z)(df_new[col])\n",
        "    plt.plot(df_new[col], y_hat, \"r--\", lw=1)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4oYzxFMenhE"
      },
      "outputs": [],
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "plt.figure(figsize=(20,15))\n",
        "sns.heatmap(abs(round(df_new.corr(),3)), annot=True, cmap=plt.cm.CMRmap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5fDf_9wCk2Z"
      },
      "source": [
        "Let's include only those features in our final dataframe that are highly impacting on the dependent variable i.e Sales. For this we are using Variance Inflation Factor technique to determine multicolinearity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26TBIhFieuUD"
      },
      "outputs": [],
      "source": [
        "# Defining a function for variance_inflation_factor\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiHL0LHLC5cG"
      },
      "source": [
        "Calculating VIF(Variance Inflation Factor) by excluding:\n",
        "1. \"Sales\" -> As it is target variable\n",
        "2. \"Store\" -> As it is StoreID and not giving any information"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PG7q3IhC-nR"
      },
      "source": [
        "We have extracted new features \"CompetitionDuration\" and \"PromoDuration\" from features \"Promo2SinceWeek\", \"Promo2SinceYear\", \"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\", \"Month\", \"Year\", \"Week\". So we can exclude them while calculating VIF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KpDOCpfe1-y"
      },
      "outputs": [],
      "source": [
        "# calculating the vif by excluding some features which are not giving any information\n",
        "calc_vif(df_new[[i for i in df_new.describe().columns if i not in [\"Store\", \"Sales\", \"Promo2SinceWeek\", \"Promo2SinceYear\", \"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\",\"Month\", \"Year\", \"Week\"]]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_xgYy2hDMce"
      },
      "source": [
        "Since, \"Promo2\",\"PromoInterval_Jan,Apr,Jul,Oct\", \"PromoInterval_Feb,May,Aug,Nov\", \"PromoInterval_Mar,Jun,Sept,Dec\" are having high VIF values and \"PromoInterval_Jan,Apr,Jul,Oct\" is having least corelation with \"Sales\". So, let's exclude \"PromoInterval_Jan,Apr,Jul,Oct\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q9P2cjUe7Bi"
      },
      "outputs": [],
      "source": [
        "# calculating the vif by excluding some features\n",
        "calc_vif(df_new[[i for i in df_new.describe().columns if i not in [\"Store\", \"Sales\", \"Promo2SinceWeek\", \"Promo2SinceYear\", \"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\", \"Month\", \"Year\",\"Week\", \"PromoInterval_Jan,Apr,Jul,Oct\"]]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXIbr739Ddjn"
      },
      "source": [
        "Since \"Assortment_b\" and \"Store_type_b\" are also showing high corelation i.e 0.72 so we are gaining same information from both the features. Its better to exclude the feature \"Assortment_b\" which is having less corelation with Sales(0.047)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogD0R8ZFfA3m"
      },
      "outputs": [],
      "source": [
        "# calculating the vif by excluding some features\n",
        "calc_vif(df_new[[i for i in df_new.describe().columns if i not in [\"Store\", \"Sales\", \"Promo2SinceWeek\", \"Promo2SinceYear\", \"CompetitionOpenSinceMonth\", \"CompetitionOpenSinceYear\", \"Month\",\"Week\", \"Year\", \"PromoInterval_Jan,Apr,Jul,Oct\",\"Assortment_b\",\"PromoDuration\"]]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRfz6RR6DtMl"
      },
      "source": [
        "Great !!! We are getting very good VIF's (Less then 10). Now let's move forward and store the selected features in a new dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRIqHNjYfF69"
      },
      "outputs": [],
      "source": [
        "#New Dataframe which will be our final dataframe\n",
        "final_df = df_new[[\"DayOfWeek\", \"Customers\",\"Promo\",\"StateHoliday\",\"SchoolHoliday\",\"CompetitionDistance\",\"Promo2\",\"StoreType_b\",\"StoreType_c\",\"StoreType_d\",\"Assortment_c\",\"PromoInterval_Feb,May,Aug,Nov\",\"PromoInterval_Mar,Jun,Sept,Dec\",\"Day\",\"CompetitionDuration\",\"Sales\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noTq8tASfJoC"
      },
      "outputs": [],
      "source": [
        "# Checking info of our final dataframe\n",
        "final_df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMng2IbBLp7"
      },
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      },
      "source": [
        "We have plotted the seaborn's scatterplot and seaborn's heatmap to see the relationship of each of the feature with target variable and observed the following correlations:\n",
        "1. **Positive Corelation**- Customers, Open, Promo, SchoolHoliday, CompetitionOpenSinceYear, Assortment_b, Assortment_c, Month, Year, Week, PromoDuration.\n",
        "2. **Negative Corelation**- DayOfWeek, StateHoliday, ComptitionDistance, CompetitionOpenSinceMonth, Promo2, Promo2SinceWeek,Promo2SinceYear, 'PromoInterval_Feb,May,Aug,Nov, 'PromoInterval_Jan,Apr,Jul,Oct, PromoInterval_Mar,Jun,Sept,Dec, Day, CompetitionDuration.\n",
        "3. **No Corelation**- Store, StoreType_c, StoreType_d\n",
        "\n",
        "We have used **Filter method** with correlation heatmap and VIF(various inflation factor) and excluded some of the features that were creating noise while model implimentation.Also, we have removed multicolinearity and selected features that are highly dependant on our target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      },
      "source": [
        "##### Which all features you found important and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGgaEstsBnaf"
      },
      "source": [
        "We have selected \"DayOfWeek\",\"Customers\",\"Promo\",\"StateHoliday\",\"SchoolHoliday\",\"CompetitionDistance\",\"Promo2\",\"StoreType_b\",\"StoreType_c\",\"StoreType_d\",\"Assortment_c\",\"PromoInterval_Feb,May,Aug,Nov\",\"PromoInterval_Mar,Jun,Sept,Dec\",\"Day\",\"CompetitionDuration\" as our final features as they are highly corelated with the target variable (Sales) and no two features are providing the same information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNVZ9zx19K6k"
      },
      "source": [
        "### 5. Data Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqoHp30x9hH9"
      },
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko4X1OOtfb4t"
      },
      "outputs": [],
      "source": [
        "# checking which of the variables are continous in nature\n",
        "for i in final_df.columns:\n",
        "  print(f\"The number of unique counts in feature {i} is: {final_df[i].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDw99AsSferC"
      },
      "outputs": [],
      "source": [
        "# Storing the continous variables(number of unique counts >40) in a separate list and treating them in order to make gaussian distribution\n",
        "cont_variables= [\"Sales\", \"Customers\", \"CompetitionDistance\", \"CompetitionDuration\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMeFoMfefhF6"
      },
      "outputs": [],
      "source": [
        "# Checking the distribution of each continous variable from our final dataframe\n",
        "plt.figure(figsize=(20,5))\n",
        "print(\"Before Applying Transformation\")\n",
        "for n,col in enumerate(cont_variables):\n",
        "  plt.subplot(1,5,n+1)\n",
        "  sns.distplot(final_df[col])\n",
        "  plt.title(f'Distribution of {col}')\n",
        "  plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NswsXE4tEjR_"
      },
      "source": [
        "It is clear from above distribution plots that they are not perfectly normally distributed. So we will apply some transformation techniques to get the normally disrtibuted data as it is one of the prior underlying assumption for the linear models such as Linear Regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ4yUIy0f0Qk"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stat\n",
        "import pylab\n",
        "#### If you want to check whether feature is guassian or normal distributed\n",
        "#### Q-Q plot\n",
        "def plot_data(df,feature):\n",
        "    stat.probplot(df[feature],dist='norm',plot=pylab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDvmBji4f4hH"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "for num,column in enumerate(cont_variables):\n",
        "  plt.subplot(1,5,num+1)\n",
        "  print(f\"Q-Q Plot for variable: {column}\")\n",
        "  plot_data(final_df,column)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bU7GjDhEwTI"
      },
      "source": [
        "As it is clear from above plots that our continous numeric features are not following perfectly normal distribution trend. So, now we will try various transformations techniques to get gaussian distributed curve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqdD3pCZEzj8"
      },
      "source": [
        "Let's create different copies and check which transformation is best for each feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOMcgiMigC7w"
      },
      "outputs": [],
      "source": [
        "# Creating 5 different copies to check the distribution of each of the variable\n",
        "test_df1=final_df.copy()\n",
        "test_df2=final_df.copy()\n",
        "test_df3=final_df.copy()\n",
        "test_df4=final_df.copy()\n",
        "test_df5=final_df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCmUMZu0E8NA"
      },
      "source": [
        "Logarithmic Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LFF2_KdlgIWx"
      },
      "outputs": [],
      "source": [
        "# Applying transformation on the above considered columns\n",
        "test_df1['Sales']=np.log(test_df1['Sales']+1)\n",
        "test_df1['Customers']=np.log(test_df1['Customers']+1)\n",
        "test_df1['CompetitionDistance']=np.log(test_df1['CompetitionDistance']+1)\n",
        "test_df1['CompetitionDuration']=np.log(test_df1['CompetitionDuration']+1)\n",
        "\n",
        "# Checking the distribution of each continous variable by excluding 0 from our final dataframe\n",
        "plt.figure(figsize=(20,5))\n",
        "print(\"After Applying Transformation\")\n",
        "for n,col in enumerate(cont_variables):\n",
        "  plt.subplot(1,5,n+1)\n",
        "  sns.distplot(test_df1[col])\n",
        "  plt.title(f'Distribution of {col}')\n",
        "  plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI4g5bWpgIof"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "for num,column in enumerate(cont_variables):\n",
        "  plt.subplot(1,5,num+1)\n",
        "  print(f\"Q-Q Plot for variable: {column}\")\n",
        "  plot_data(test_df1,column)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPR3xurgFOdw"
      },
      "source": [
        "Reciprocal Trnasformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeJXBn6Rgz8P"
      },
      "outputs": [],
      "source": [
        "# Applying transformation on the above considered columns\n",
        "test_df2['Sales']=1/(test_df2['Sales']+1)\n",
        "test_df2['Customers']=1/(test_df2['Customers']+1)\n",
        "test_df2['CompetitionDistance']=1/(test_df2['CompetitionDistance']+1)\n",
        "test_df2['CompetitionDuration']=1/(test_df2['CompetitionDuration']+1)\n",
        "\n",
        "\n",
        "# Checking the distribution of each continous variable by excluding 0 from our final dataframe\n",
        "plt.figure(figsize=(20,5))\n",
        "print(\"After Applying Transformation\")\n",
        "for n,col in enumerate(cont_variables):\n",
        "  plt.subplot(1,5,n+1)\n",
        "  sns.distplot(test_df2[col])\n",
        "  plt.title(f'Distribution of {col}')\n",
        "  plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqKiZwp6g6Ur"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "for num,column in enumerate(cont_variables):\n",
        "  plt.subplot(1,5,num+1)\n",
        "  print(f\"Q-Q Plot for variable: {column}\")\n",
        "  plot_data(test_df2,column)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgllxY4TFd07"
      },
      "source": [
        "Square Root Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vIH3exfg_Vs"
      },
      "outputs": [],
      "source": [
        "# Applying transformation on the above considered columns\n",
        "test_df3['Sales']=(test_df3['Sales'])**(1/2)\n",
        "test_df3['Customers']=(test_df3['Customers'])**(1/2)\n",
        "test_df3['CompetitionDistance']=(test_df3['CompetitionDistance'])**(1/2)\n",
        "test_df3['CompetitionDuration']=(test_df3['CompetitionDuration'])**(1/2)\n",
        "\n",
        "# Checking the distribution of each continous variable by excluding 0 from our final dataframe\n",
        "plt.figure(figsize=(20,5))\n",
        "print(\"After Applying Transformation\")\n",
        "for n,col in enumerate(cont_variables):\n",
        "  plt.subplot(1,5,n+1)\n",
        "  sns.distplot(test_df3[col])\n",
        "  plt.title(f'Distribution of {col}')\n",
        "  plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmCKgTVZhD-4"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "for num,column in enumerate(cont_variables):\n",
        "  plt.subplot(1,5,num+1)\n",
        "  print(f\"Q-Q Plot for variable: {column}\")\n",
        "  plot_data(test_df3,column)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM3VNORvFtcz"
      },
      "source": [
        "Exponential Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzDGlIYEhI8e"
      },
      "outputs": [],
      "source": [
        "# Applying transformation on the above considered columns\n",
        "test_df4['Sales']=(test_df4['Sales'])**(1/1.2)\n",
        "test_df4['Customers']=(test_df4['Customers'])**(1/1.2)\n",
        "test_df4['CompetitionDistance']=(test_df4['CompetitionDistance'])**(1/1.2)\n",
        "test_df4['CompetitionDuration']=(test_df4['CompetitionDuration'])**(1/1.2)\n",
        "\n",
        "# Checking the distribution of each continous variable by excluding 0 from our final dataframe\n",
        "plt.figure(figsize=(20,5))\n",
        "print(\"After Applying Transformation\")\n",
        "for n,col in enumerate(cont_variables):\n",
        "  plt.subplot(1,5,n+1)\n",
        "  sns.distplot(test_df4[col])\n",
        "  plt.title(f'Distribution of {col}')\n",
        "  plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfMYD112hMxZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20,5))\n",
        "for num,column in enumerate(cont_variables):\n",
        "  plt.subplot(1,5,num+1)\n",
        "  print(f\"Q-Q Plot for variable: {column}\")\n",
        "  plot_data(test_df4,column)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmAg6sx8F8qX"
      },
      "source": [
        "From the above plots it is clear that:\n",
        "1. Feature \"Sales\" needs square root transformation\n",
        "2. Feature \"Customers\" needs square root transformation\n",
        "3. Feature \"CompetitionDistance\" needs square root transformation\n",
        "4. Feature \"CompetitionDuration\" no transformation needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmvTFy3LhUD2"
      },
      "outputs": [],
      "source": [
        "# Applying transformation on the above considered columns\n",
        "## Square root transformation\n",
        "final_df['Sales']=(final_df['Sales'])**(1/2)\n",
        "final_df['Customers']=(final_df['Customers'])**(1/2)\n",
        "final_df['CompetitionDistance']=(final_df['CompetitionDistance'])**(1/2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH9lsjgGGDV2"
      },
      "source": [
        "Now we have normally distributed data for all of the continous variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMDnDkt2B6du"
      },
      "source": [
        "### 6. Data Scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o7RpKdIGIRt"
      },
      "source": [
        "Before scaling our data let's just seperate our \"x\" and \"y\" variables as we do not have to scale our y variable (Target variable)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0th57bGvhc_g"
      },
      "outputs": [],
      "source": [
        "# Separating \"x\" and \"y\" variables\n",
        "x= final_df[[\"DayOfWeek\", \"Customers\",\"Promo\",\"StateHoliday\",\"SchoolHoliday\",\"CompetitionDistance\",\"Promo2\",\"StoreType_b\",\"StoreType_c\",\"StoreType_d\",\"Assortment_c\",\"PromoInterval_Feb,May,Aug,Nov\",\"PromoInterval_Mar,Jun,Sept,Dec\",\"Day\",\"CompetitionDuration\"]]\n",
        "y= final_df[['Sales']]\n",
        "print(x.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QpE_nkBhgHc"
      },
      "outputs": [],
      "source": [
        "# Scaling your data\n",
        "# Importing StandardScaler library\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZJPvH1bhh95"
      },
      "outputs": [],
      "source": [
        "# Creating object\n",
        "std_regressor= StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9HlhfxDhj5x"
      },
      "outputs": [],
      "source": [
        "# Fit and Transform\n",
        "x= std_regressor.fit_transform(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiiVWRdJDDil"
      },
      "source": [
        "##### Which method have you used to scale you data and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPYHFeWtGdq6"
      },
      "source": [
        "We have used StandardScaler of sklearn library to scale our data. This is important for us, as features on different scales can lead to poor performance or slow convergence. Standardizing the features also makes it easier to compare different features or observe the effect of a feature on the target variable(\"Sales\") by comparing the magnitude of its coefficient. Additionally, we are going to apply linear regression model for which having normally distributed data is the statistical assumption of the model, which standardization can help enforce."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UUpS68QDMuG"
      },
      "source": [
        "### 7. Dimesionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kexQrXU-DjzY"
      },
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cG0XAo5yhwui"
      },
      "outputs": [],
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "## Not needed for now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5CmagL3EC8N"
      },
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1lIgcrwh22m"
      },
      "outputs": [],
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "## Not needed for now"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhH2vgX9EjGr"
      },
      "source": [
        "### 8. Data Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2J6tvB2h8cI"
      },
      "outputs": [],
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Importing train_test_split from sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrXM3-pXh-ap"
      },
      "outputs": [],
      "source": [
        "# Spliting the dataset\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9jYm3B1iCQJ"
      },
      "outputs": [],
      "source": [
        "# Checking the shape after spliting\n",
        "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjKvONjwE8ra"
      },
      "source": [
        "##### What data splitting ratio have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      },
      "source": [
        "Since our dataset is huge and have nearly 10lakh obsevations. So, We have assigned 80% data into train set and 20% into the test set with random_state=0 so that we do not get different observations in every split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1XJ9OREExlT"
      },
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOzZv6IFROw"
      },
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeKDIv7pFgcC"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLvprIZsiRc9"
      },
      "outputs": [],
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIqpNgepFxVj"
      },
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbet1HwdGDTz"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCC591jGiD4"
      },
      "source": [
        "## ***7. ML Model Implementation***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGC8IlqyiaCO"
      },
      "outputs": [],
      "source": [
        "# Importing essential libraries to check the accuracy\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_absolute_percentage_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtW3AOlvicd7"
      },
      "outputs": [],
      "source": [
        "# Defining the function that calculated regression metrics\n",
        "def regression_metrics(y_train_actual,y_train_pred,y_test_actual,y_test_pred):\n",
        "  print(\"-\"*50)\n",
        "  ## mean_absolute_error\n",
        "  MAE_train= mean_absolute_error(y_train,y_train_pred)\n",
        "  print(\"MAE on train is:\" ,MAE_train)\n",
        "  MAE_test= mean_absolute_error(y_test,y_test_pred)\n",
        "  print(\"MAE on test is:\" ,MAE_test)\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  ## mean_squared_error\n",
        "  MSE_train= mean_squared_error(y_train, y_train_pred)\n",
        "  print(\"MSE on train is:\" ,MSE_train)\n",
        "  MSE_test  = mean_squared_error(y_test, y_test_pred)\n",
        "  print(\"MSE on test is:\" ,MSE_test)\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  ## root_mean_squared_error\n",
        "  RMSE_train = np.sqrt(MSE_train)\n",
        "  print(\"RMSE on train is:\" ,RMSE_train)\n",
        "  RMSE_test = np.sqrt(MSE_test)\n",
        "  print(\"RMSE on test is:\" ,RMSE_test)\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  ## mean_absolute_percentage_error\n",
        "  MAPE_train = mean_absolute_percentage_error(y_train, y_train_pred)*100\n",
        "  print(\"MAPE on train is:\" ,MAPE_train, \" %\")\n",
        "  MAPE_test = mean_absolute_percentage_error(y_test, y_test_pred)*100\n",
        "  print(\"MAPE on test is:\" ,MAPE_test, \" %\")\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  ## r2_score\n",
        "  R2_train= r2_score(y_train,y_train_pred)\n",
        "  print(\"R2 on train is:\" ,R2_train)\n",
        "  R2_test= r2_score(y_test,y_test_pred)\n",
        "  print(\"R2 on test is:\" ,R2_test)\n",
        "\n",
        "  print(\"-\"*50)\n",
        "\n",
        "  Accuracy_train= 100- MAPE_train\n",
        "  print(\"Accuracy of train is:\" ,Accuracy_train, \" %\")\n",
        "  Accuracy_test= 100- MAPE_test\n",
        "  print(\"Accuracy of test is:\" ,Accuracy_test, \" %\")\n",
        "\n",
        "  print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      },
      "source": [
        "### ML Model - 1 - **Linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlfSdc98ig68"
      },
      "outputs": [],
      "source": [
        "# Importing LinearRegression from sklearn\n",
        "from sklearn.linear_model import LinearRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxdDXIklijmb"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation\n",
        "linear_regressor= LinearRegression()\n",
        "\n",
        "# Fit the Algorithm\n",
        "linear_regressor.fit(x_train,y_train)\n",
        "\n",
        "# Predict the model\n",
        "y_train_regression_pred= linear_regressor.predict(x_train)\n",
        "y_test_regression_pred= linear_regressor.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fbpJq4bimD8"
      },
      "outputs": [],
      "source": [
        "# Checking the coefficients\n",
        "linear_regressor.coef_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NY8sjrSEin1L"
      },
      "outputs": [],
      "source": [
        "# Checking the intercept\n",
        "linear_regressor.intercept_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArJBuiUVfxKd"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOMqAAUpisLQ"
      },
      "outputs": [],
      "source": [
        "# Calculating the regression metrics\n",
        "regression_metrics(y_train,y_train_regression_pred,y_test,y_test_regression_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xvncAl8pivob"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculating residuals\n",
        "residuals = y_test - y_test_regression_pred\n",
        "Mean= round(np.mean(residuals),2)\n",
        "Median= round(np.median(residuals),2)\n",
        "\n",
        "# Plot residuals\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.scatter(y_test, residuals, c=\"dodgerblue\")\n",
        "plt.title(\"Residual Plot for Metric Evaluation\")\n",
        "plt.xlabel('Predicted Sales')\n",
        "plt.ylabel('Residual Error')\n",
        "\n",
        "# Add horizontal line at mean value of y\n",
        "plt.axhline(y=np.nanmean(residuals), color='red', linestyle='--', label=Mean[0])\n",
        "plt.axhline(y=np.nanmedian(residuals), color='green', linestyle='--', label=Median)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNOxCR8lIBDX"
      },
      "source": [
        "We have started with the most basic and simple ML model i.e Linear Regression. We have tried to evaluate the most important regression metics on both the train and test datesets so that we can conclude our ML model. Here for Linear Regression, we can observe that both the r2 scores are pretty close which explains that on test dataset and our model is following the correct way.\n",
        "\n",
        "\n",
        "We can comprehend that 'dependent' and 'independent' variables are not much following direct linear relationship with each other thats why we got 0.75 maximum r2 score in LR model implementation.\n",
        "\n",
        "Also we are getting the Mean=0.03 and Median=-0.33 in our residual plot that resembles our predictions are closely following Normal Distribution (Mean=Median=0) because of this our accuracy (100-MAPE) is pretty good in simple linear model.\n",
        "\n",
        "In order to fetch good and more accurate results, we shall go for cross- Validation & Hyperparameter Tuning of 'Lasso', 'Ridge' and 'Elastic Net' models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qY1EAkEfxKe"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dtiG-HWJDFe"
      },
      "source": [
        "Rigde Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbXtwt-vi30p"
      },
      "outputs": [],
      "source": [
        "# import ridge regression from sklearn library\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating Ridge instance\n",
        "ridge= Ridge()\n",
        "\n",
        "# Defining parameters\n",
        "parameters = {\"alpha\": [1e-1,1,5,7,10,11,14,15,16,17], \"max_iter\":[1,2,3]}\n",
        "\n",
        "# Train the model\n",
        "ridgeR = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "ridgeR.fit(x_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_ridge_pred = ridgeR.predict(x_train)\n",
        "y_test_ridge_pred = ridgeR.predict(x_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {ridgeR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {ridgeR.best_score_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4GGOYz3i6iC"
      },
      "outputs": [],
      "source": [
        "# Calculating regression metrics for Ridge\n",
        "regression_metrics(y_train,y_train_ridge_pred,y_test,y_test_ridge_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs6m_tMRJX4g"
      },
      "source": [
        "Lasso Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFlMAZB9i-6M"
      },
      "outputs": [],
      "source": [
        "# import lasso regression from sklearn library\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating Ridge instance\n",
        "lasso= Lasso()\n",
        "\n",
        "# Defining parameters\n",
        "parameters_lasso = {\"alpha\": [1e-5,1e-4,1e-3,1e-2,1e-1,1,5], \"max_iter\":[7,8,9,10]}\n",
        "\n",
        "# Train the model\n",
        "lassoR = GridSearchCV(lasso, parameters_lasso, scoring='neg_mean_squared_error', cv=5)\n",
        "lassoR.fit(x_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_lasso_pred = lassoR.predict(x_train)\n",
        "y_test_lasso_pred = lassoR.predict(x_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {lassoR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {lassoR.best_score_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQhi_gysjBpL"
      },
      "outputs": [],
      "source": [
        "# Calculating regression metrics for Lasso\n",
        "regression_metrics(y_train,y_train_lasso_pred,y_test,y_test_lasso_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tulaPDoLKBJG"
      },
      "source": [
        "Elastic Net Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZaYo217jGO2"
      },
      "outputs": [],
      "source": [
        "# import elastic net regression from sklearn library\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating e_net instance\n",
        "e_net= ElasticNet()\n",
        "\n",
        "# Defining hyperparameters\n",
        "parameters_e_net = {\"alpha\": [1e-5,1e-4,1e-3,1e-2,1,5], \"max_iter\":[12,13,14,15]}\n",
        "\n",
        "# Train the model\n",
        "e_netR = GridSearchCV(e_net, parameters_e_net, scoring='neg_mean_squared_error', cv=5)\n",
        "e_netR.fit(x_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_e_net_pred = e_netR.predict(x_train)\n",
        "y_test_e_net_pred = e_netR.predict(x_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {e_netR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {e_netR.best_score_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YH0pg1lkjJS3"
      },
      "outputs": [],
      "source": [
        "# Calculating regression metrics for Elastic Net\n",
        "regression_metrics(y_train,y_train_e_net_pred,y_test,y_test_e_net_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "negyGRa7fxKf"
      },
      "source": [
        "We have used GridSearchCV as the hyperparameter optimization technique as it uses all possible combinations of hyperparameters and their values. It then calculates the performance for each combination and selects the best value for the hyperparameters. This offers the most accurate tuning method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfvqoZmBfxKf"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaLui8CcfxKf"
      },
      "source": [
        "Despite using Lasso, Ridge and Elastic net models, we couldn't see any significant improvement in the r2 score, MSE and on MAPE as well. This provoked us to go for higher and more complex ML models like Decision trees, Random Forest, XGBoost Regression and LightGBM Regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      },
      "source": [
        "### ML Model - 2 - **Decision Trees**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbKALFmrK7-e"
      },
      "source": [
        "Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUn3cxqOjYDh"
      },
      "outputs": [],
      "source": [
        "# import the regressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# create a regressor object\n",
        "TreeR = DecisionTreeRegressor(max_depth=16)\n",
        "\n",
        "# fit the regressor with X and Y data\n",
        "TreeR.fit(x_train, y_train)\n",
        "\n",
        "# predict the model\n",
        "y_train_tree_pred= TreeR.predict(x_train)\n",
        "y_test_tree_pred= TreeR.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xWym5PkjaHS"
      },
      "outputs": [],
      "source": [
        "# Calculating Regression Metrics\n",
        "regression_metrics(y_train,y_train_tree_pred,y_test,y_test_tree_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19DtaRqfjc4h"
      },
      "outputs": [],
      "source": [
        "# Importing libraries for visualizing decison tree\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn import tree\n",
        "from IPython.display import SVG\n",
        "from graphviz import Source\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing the Decision tree\n",
        "#plt.figure(figsize=(5,50))\n",
        "#graph = Source(tree.export_graphviz(TreeR, out_file=None, feature_names=final_df.columns[:-1], class_names=['0', '1'] , filled = True))\n",
        "#display(SVG(graph.pipe(format='svg')))"
      ],
      "metadata": {
        "id": "Ldvv1tfbLWHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYfwnehpsJ1"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjcsCUNtjkP0"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculating residuals\n",
        "y_test_tree_pred= y_test_tree_pred.reshape(-1,1)\n",
        "residuals_DT = y_test - y_test_tree_pred\n",
        "Mean= round(np.mean(residuals_DT),2)\n",
        "Median= round(np.median(residuals_DT),2)\n",
        "\n",
        "# Plot residuals\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.scatter(y_test, residuals_DT, c=\"lightgreen\")\n",
        "plt.title(\"Residual Plot for Metric Evaluation\")\n",
        "plt.xlabel('Predicted Sales')\n",
        "plt.ylabel('Residual Error')\n",
        "\n",
        "# Add horizontal line at mean value of y\n",
        "plt.axhline(y=np.nanmean(residuals_DT), color='red', linestyle='--', label=Mean[0])\n",
        "plt.axhline(y=np.nanmedian(residuals_DT), color='green', linestyle='--', label=Median)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2nG6lQbMesQ"
      },
      "source": [
        "After simple LR models, we tried 'Decision Tree' and we saw a good increment in the r2 score from 0.75 to 0.90 that means \"90% Variance of our test dataset is captured by our trained model\" which is excellent. On the other side our RMSE also decreased and shifted below 5(=4.7) which is very good.Also accuracy increased from 93% to 95%. On the other hand from the residual plot our values of mean and median are shifting towards 0 that means our model is improving. But, in the quest of more accurate and real predictions, we decided to further tune the hyperparameters and check the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDyoz0pjMjJl"
      },
      "source": [
        "Decision Tree with GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRuXle5bjtNO"
      },
      "outputs": [],
      "source": [
        "# import ridge regression from sklearn library\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating Ridge instance\n",
        "decision_tree= DecisionTreeRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters= {'max_depth': [16,17,18], 'min_samples_leaf': [6,7,8], 'min_samples_split': [1,2,4]}\n",
        "\n",
        "# Train the model\n",
        "decision_treeR = GridSearchCV(decision_tree, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "decision_treeR.fit(x_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_grid_Dtree_pred = decision_treeR.predict(x_train)\n",
        "y_test_grid_Dtree_pred = decision_treeR.predict(x_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {decision_treeR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {decision_treeR.best_score_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L7CV2mGEjwAU"
      },
      "outputs": [],
      "source": [
        "# Calculating Regression Metrics\n",
        "regression_metrics(y_train,y_train_grid_Dtree_pred,y_test,y_test_grid_Dtree_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAih1iBOpsJ2"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      },
      "source": [
        "We have used GridSearchCV as the hyperparameter optimization technique as it uses all possible combinations of hyperparameters and provides the more accurate results. It then calculates the performance for each combination and selects the best value for the hyperparameters. This offers the most accurate tuning method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74yRdG6UpsJ3"
      },
      "source": [
        "We have used different combinations of parameters to get the best value of r2 score and least MAPE for our case. The best combination was found out to be {'max_depth': 18, 'min_samples_leaf': 7, 'min_samples_split': 4} which resulted into the improvement in the MSE from 22% to 19% on the test set. Also MAPE is further reduced from 4% to 3% and capturing variance 1% more i.e 91% of the test dataset. At this point of time we have achieved above 95% (=96.30%) accuracy by hyperparameter tuning of Decision trees."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      },
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      },
      "source": [
        "In order to minimise the errors between actual and predicted values, we evaluate our ML model using different metrics. All these metrics try to give us an indication on how close we are with the real/expected output. In our case, each evaluation metric is showing not much difference on the train and test data which shows that our model is predicting a closer expected value. So the sales, the dependent variable, which impacts the business is getting accurately predicted to the extent of ~96% and ~3% far from the mean of actual absolute values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fze-IPXLpx6K"
      },
      "source": [
        "### ML Model - 3 - **Random Forest Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za1tDWqGN0-V"
      },
      "source": [
        "Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZcoxLz1kDz_"
      },
      "outputs": [],
      "source": [
        "# import the regressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "# create a regressor object\n",
        "RF_TreeR = RandomForestRegressor(n_estimators=100, max_depth=18)\n",
        "\n",
        "# fit the regressor with X and Y data\n",
        "RF_TreeR.fit(x_train, y_train)\n",
        "\n",
        "# predict the model\n",
        "y_train_RFtree_pred= RF_TreeR.predict(x_train)\n",
        "\n",
        "y_test_RFtree_pred= RF_TreeR.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aeiSEoHbkHJO"
      },
      "outputs": [],
      "source": [
        "# Calculating Regression Metrics using RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_RFtree_pred,y_test,y_test_RFtree_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AN1z2sKpx6M"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sphzKJNkL8G"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculating residuals\n",
        "y_test_RFtree_pred= y_test_RFtree_pred.reshape(-1,1)\n",
        "residuals_RF = y_test - y_test_RFtree_pred\n",
        "Mean= round(np.mean(residuals_RF),2)\n",
        "Median= round(np.median(residuals_RF),2)\n",
        "\n",
        "# Plot residuals\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.scatter(y_test, residuals_DT, c=\"tomato\")\n",
        "plt.title(\"Residual Plot for Metric Evaluation\")\n",
        "plt.xlabel('Predicted Sales')\n",
        "plt.ylabel('Residual Error')\n",
        "\n",
        "# Add horizontal line at mean value of y\n",
        "plt.axhline(y=np.nanmean(residuals_RF), color='red', linestyle='--', label=Mean[0])\n",
        "plt.axhline(y=np.nanmedian(residuals_RF), color='green', linestyle='--', label=Median)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RT7kUkJzPjDj"
      },
      "source": [
        "By implimenting using our third model i.e Random Forest we have achieved the r2 score of 0.95 on training and 0.93 on test dataset that is very good MSE also reduced from 19 to 17 and that means our model is moving towards optimal model. Apart from this our MAPE is also reducing gradually as we are using ensemble of Decision trees that is Bootstrap Aggregation(Bagging) so it is giving equal preference each of the feature while spliting.\n",
        "\n",
        "We got the Mean=0.02 and Median=-0.09 this shows that as our accuracy increases, our mean and median are shifting towards 0 and the residual error is tending toward gaussian distribution.\n",
        "\n",
        "In order to get the higher accuracy let's perform hyperparameter tuning for the same model and see if we are getting significant results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PIHJqyupx6M"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-mAh604PtAI"
      },
      "source": [
        "Random Forest with RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iADZF-99AAiv"
      },
      "outputs": [],
      "source": [
        "# import ridge regression from sklearn library\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating Ridge instance\n",
        "decision_tree= DecisionTreeRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters= {'max_depth': [16,17,18], 'min_samples_leaf': [6,7,8], 'min_samples_split': [1,2,4]}\n",
        "\n",
        "# Train the model\n",
        "decision_treeR = GridSearchCV(decision_tree, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "decision_treeR.fit(x_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_grid_Dtree_pred = decision_treeR.predict(x_train)\n",
        "y_test_grid_Dtree_pred = decision_treeR.predict(x_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {decision_treeR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {decision_treeR.best_score_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9aw5_MJkUrk"
      },
      "outputs": [],
      "source": [
        "# import ridge regression from sklearn library\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Creating Ridge instance\n",
        "RF_tree= RandomForestRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters= {'n_estimators':[100], 'max_depth': [19,20], 'min_samples_leaf': [1, 2]}\n",
        "\n",
        "# Train the model\n",
        "RF_treeR = RandomizedSearchCV(RF_tree, parameters, n_iter=3, n_jobs=-1, scoring='neg_mean_squared_error', cv=3,  verbose=3)\n",
        "RF_treeR.fit(x_train, y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_grid_RFtree_pred = RF_treeR.predict(x_train)\n",
        "y_test_grid_RFtree_pred = RF_treeR.predict(x_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {RF_treeR.best_params_}\")\n",
        "print(f\"Negative mean square error is: {RF_treeR.best_score_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z8VzX7TfkXcX"
      },
      "outputs": [],
      "source": [
        "# Calculating Regression Metrics using GridSearchCV in RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_grid_RFtree_pred,y_test,y_test_grid_RFtree_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-qAgymDpx6N"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQMffxkwpx6N"
      },
      "source": [
        "We have used RandomizedSearchCV in Random Forest since we have huge dataset and it is good for huge and complex models where we just want to select random parameters from the bag of parameters. It reduces the processing and training time by taking the random subsets of the provided parameters wihout compromising the accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hykwinpx6N"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzVzZC6opx6N"
      },
      "source": [
        "Answer Here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_CCil-SKHpo"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHVz9hHDKFms"
      },
      "source": [
        "After using RandomizedSearchCV with different hyperparameters we saw that their is not much significant improvement observed. Although MSE on test dataset has been reduced from 14 to 13."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrjvmBEzZoE1"
      },
      "source": [
        "### ML Model - 4 - **LightGBM Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3THwHackZvd4"
      },
      "source": [
        "LightGBM Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdadF4OGkrIP"
      },
      "outputs": [],
      "source": [
        "# import the regressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# create a regressor object\n",
        "lgbmR = LGBMRegressor(boosting_type='gbdt', max_depth=120, learning_rate=0.1, n_estimators=500,  n_jobs=-1)\n",
        "\n",
        "# fit the regressor with X and Y data\n",
        "lgbmR.fit(x_train, y_train)\n",
        "\n",
        "# predict the model\n",
        "y_train_lgbmR_pred= lgbmR.predict(x_train)\n",
        "y_test_lgbmrR_pred= lgbmR.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdmNEweIkvdd"
      },
      "outputs": [],
      "source": [
        "# Calculating Regression Metrics using RandomForestRegressor\n",
        "regression_metrics(y_train, y_train_lgbmR_pred, y_test, y_test_lgbmrR_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo-S2tR9aPZe"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISgDuYg2kzyN"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculating residuals\n",
        "y_test_lgbmrR_pred= y_test_lgbmrR_pred.reshape(-1,1)\n",
        "residuals_LGBM = y_test - y_test_lgbmrR_pred\n",
        "Mean= round(np.mean(residuals_LGBM),2)\n",
        "Median= round(np.median(residuals_LGBM),2)\n",
        "\n",
        "# Plot residuals\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.scatter(y_test, residuals_DT, c=\"gold\")\n",
        "plt.title(\"Residual Plot for Metric Evaluation\")\n",
        "plt.xlabel('Predicted Sales')\n",
        "plt.ylabel('Residual Error')\n",
        "\n",
        "# Add horizontal line at mean value of y\n",
        "plt.axhline(y=np.nanmean(residuals_LGBM), color='red', linestyle='--', label=Mean[0])\n",
        "plt.axhline(y=np.nanmedian(residuals_LGBM), color='green', linestyle='--', label=Median)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR4J4jaWaZNb"
      },
      "source": [
        "LightGBM is the lighter version of GBM. It has more faster and accurate than other popular gradient boosting libraries such as XGBoost on several datasets. We want to check if our accuracy score can be improved further, so we have tried implimenting LightGBM in order to achieve more accurate results.\n",
        "\n",
        "We saw that with the help of LightGBM we are able to capture 92% of the Variance of the dependent varibale with the help of independent variables(r2 score) for testing dataset and achieved 96% accuracy. Also we are getting MAPE of 3% which is similar as Random Forest.\n",
        "\n",
        "By plotting the residual plot we are getting Mean=0.02 and Median=-0.12 and it denotes that residuals are close to normal distribution.\n",
        "\n",
        "We have further checked the performance metrics by hyperparameter tuning of LightGBM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJIqzGxZadf9"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lA1jk8PYahTI"
      },
      "source": [
        "LightGBM with RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0RWlIdFk87Z"
      },
      "outputs": [],
      "source": [
        "# import ridge regression from sklearn library and RandomizedSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# Creating XGBoost instance\n",
        "lgbm= LGBMRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters={\"learning_rate\":[0.01,0.1],\"max_depth\":[120,125,150],\"n_estimators\":[500,600]}\n",
        "\n",
        "# Train the model\n",
        "lgbm_rand_R= RandomizedSearchCV(lgbm,parameters,scoring='neg_mean_squared_error',n_jobs=-1,cv=3,verbose=3)\n",
        "lgbm_rand_R.fit(x_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_rand_lgbm_pred = lgbm_rand_R.predict(x_train)\n",
        "y_test_rand_lgbm_pred = lgbm_rand_R.predict(x_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {lgbm_rand_R.best_params_}\")\n",
        "print(f\"Negative mean square error is: {lgbm_rand_R.best_score_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGmlNzc_k_wM"
      },
      "outputs": [],
      "source": [
        "# Calculating Regression Metrics using GridSearchCV in RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_rand_lgbm_pred,y_test,y_test_rand_lgbm_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3XKIMqTeYAW"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPQEHTgdedS0"
      },
      "source": [
        "RandomizedSearchCV was still the better option since it is taking very less processing time without compromising the accuracy. So we have mutually decided to use that hyperparameter optimization technique."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2tjMyxSehOM"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ediTwy3BekRV"
      },
      "source": [
        "We have tried different parameters for tuning of our LightGBM model and achieved 0.92 r2 score on training dataset, 0.92 on testing set as well that means our model is optimized and not falling under the underfitting or overfitting side. Also, our MAPE is reduced from 7% to 6% after hyperparameter optimization which is fair enough and improved. The best parameters obtained by the optimatization is {'n_estimators': 600, 'max_depth': 150, 'learning_rate': 0.1}."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKCHjszaevBs"
      },
      "source": [
        "### ML Model - 5 - **XGBoost Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt0VcUILeyTg"
      },
      "source": [
        "XGBoost Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWtSi3PWlOK8"
      },
      "outputs": [],
      "source": [
        "# import the regressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "# create a regressor object\n",
        "xgbR = XGBRegressor(learning_rate=0.2, max_depth=10)\n",
        "\n",
        "# fit the regressor with X and Y data\n",
        "xgbR.fit(x_train, y_train)\n",
        "\n",
        "# predict the model\n",
        "y_train_xgbR_pred= xgbR.predict(x_train)\n",
        "y_test_xgbR_pred= xgbR.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrcuYkgRlQ4k"
      },
      "outputs": [],
      "source": [
        "# Calculating Regression Metrics using RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_xgbR_pred,y_test,y_test_xgbR_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWVRb3NFfqv0"
      },
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RB-raKi9lVDC"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculating residuals\n",
        "y_test_xgbR_pred= y_test_xgbR_pred.reshape(-1,1)\n",
        "residuals_XG = y_test - y_test_xgbR_pred\n",
        "Mean= round(np.mean(residuals_XG),2)\n",
        "Median= round(np.median(residuals_XG),2)\n",
        "\n",
        "# Plot residuals\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.scatter(y_test, residuals_DT, c=\"mediumslateblue\")\n",
        "plt.title(\"Residual Plot for Metric Evaluation\")\n",
        "plt.xlabel('Predicted Sales')\n",
        "plt.ylabel('Residual Error')\n",
        "\n",
        "# Add horizontal line at mean value of y\n",
        "plt.axhline(y=np.nanmean(residuals_XG), color='red', linestyle='--', label=Mean[0])\n",
        "plt.axhline(y=np.nanmedian(residuals_XG), color='green', linestyle='--', label=Median)\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhsPnkW2fzRi"
      },
      "source": [
        "XGBoost (eXtreme Gradient Boosting) is a Gradiant boosting algorithm and very popular for achieving good accuracies. We have used XGBoost in order to shift the MAPE towards 0 (minimize the error).\n",
        "\n",
        "We got the Mean=0.0 and Median=-0.09 which also super close to 0 that means our residuals are perfectly following normal distribution against Sales.\n",
        "\n",
        "We got the MAPE of 3% and r2 score of 0.94 for testing dataset which is which is excelent and improved as well. At this point of time slightly improvement in MAPE can lead to huge profit to stakeholders and we were very curious and excited at this point of time to further improve the efficiency of our model and for this we have again decided to tune the various hyperparameters of xgboost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfAZoOGpf2qb"
      },
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHjgmivLf6au"
      },
      "source": [
        "XGBoost with GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNiBH-uZle0n"
      },
      "outputs": [],
      "source": [
        "# XGBoost with RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating XGBoost instance\n",
        "xgb= XGBRegressor()\n",
        "\n",
        "# Defining parameters\n",
        "parameters={\"learning_rate\":[0.01, 0.1],\"max_depth\":[12,13]}\n",
        "\n",
        "# Train the model\n",
        "xgb_Rand_R= GridSearchCV(xgb,parameters,scoring='neg_mean_squared_error',n_jobs=-1,cv=3,verbose=3)\n",
        "xgb_Rand_R.fit(x_train,y_train)\n",
        "\n",
        "# Predict the output\n",
        "y_train_rand_xgbR_pred = xgb_Rand_R.predict(x_train)\n",
        "y_test_rand_xgbR_pred = xgb_Rand_R.predict(x_test)\n",
        "\n",
        "# Printing the best parameters obtained by GridSearchCV\n",
        "print(f\"The best alpha value found out to be: {xgb_Rand_R.best_params_}\")\n",
        "print(f\"Negative mean square error is: {xgb_Rand_R.best_score_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJIOHMl8lh3u"
      },
      "outputs": [],
      "source": [
        "# Calculating Regression Metrics using GridSearchCV in RandomForestRegressor\n",
        "regression_metrics(y_train,y_train_rand_xgbR_pred,y_test,y_test_rand_xgbR_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fi3GG4q7n4cV"
      },
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edMOmZgkn8Hc"
      },
      "source": [
        "XGboost is a heavy algorithm and takes much processing time with GridSearchCV. So, tuning of hyperparameter with GridSearchCV was a bit complicated task for us but we have used less parameterts in GridSearchCV because we did'nt want to miss the best parameter combination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrgGDZben_wZ"
      },
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JWYIfdSoEFO"
      },
      "source": [
        "Minor improvement in regresson metrics are also significant now as we are moving towards model perfection. With the help of GridSearchCV we got the r2 score of 0.94 (Now 94% of the variance of test set our model is capturing) for test dataset which is 1% higher than without RandomizedSearchCV and the best parameters found out to be{'learning_rate': 0.1, 'max_depth': 13}. Also we have noticed that our MAPE is further reduced and falling under 3% (Minimum error among all models) and on the other hand MSE is also reduced to 12%. We have also seen that on further increasing the max_depth of tree our model is overfitting so above values of parameters are the best combinations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVvpIpaSoKW8"
      },
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjAnG8x6oQ5Q"
      },
      "source": [
        "Since predicting sales over a period of time falls under the category of \"Time series data\" and there are following regression metrics that are required as per our goal of analysis (Predicting future Sales):\n",
        "\n",
        "1. MAE(Mean Absolute Error): This metric calculates the average magnitude of the errors in the predictions, without considering their direction. It has the inverse relation with the accuracy of the model. In regression analysis our aim is to minimise the MAE and ultimately this will create positive business impact.\n",
        "\n",
        "2. RMSE(Root Mean Squared Error): It is the square root of MSE and this is the most widely use regression metric since it has the same units as the original data so it is easy to interpret the magnitude of error.\n",
        "\n",
        "3. R2_Score: R2 score(coefficient of determination) is a metric that is widely used in regression analysis because it measures the proportion of the variance in the dependent variable that is explained by the independent variables. R2 score allows analysts to quickly and easily evaluate the goodness of fit of a model and compare different models. It also provides a clear measure of how well the model is explaining the variance in the dependent variable, which can aid in making decisions about model selection and further analysis.\n",
        "\n",
        "4. MAPE(Mean Absolute Percentage Error): It is calculated by taking the average of the absolute percentage differences between the predicted values and the actual values. This metric is particularly useful when working with time series data(as in our case), as it allows for easy comparison of forecast accuracy across different scales. With the help of MAPE an analyst can easily explain the percentage error to the stakeholders. This metric is considered as one of the most important regression metric in time series data for a positive business impact.\n",
        "\n",
        "5. Accuracy: In time series data(predicting Sales, Customers, Stock prices, etc) the best metric to calculate the accuracy is 100-MAPE, which is the average of the absolute percentage differences between the predicted values and the actual values. A lower value for 100-MAPE indicates a more accurate model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBFFvTBNJzUa"
      },
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQnDoNx9lzrS"
      },
      "outputs": [],
      "source": [
        "# Storing different regression metrics in order to make dataframe and compare them\n",
        "models = [\"Linear_regression\",\"Decision_tree\",\"Random_forest\",\"LightGBM\",\"XGboost\"]\n",
        "MAE_r = [5.38,2.94,2.43,2.83,2.37]\n",
        "MSE_r = [51.54,17.87,12.47,15.17,11.64]\n",
        "RMSE_r = [7.17,4.22,3.53,3.89,3.41]\n",
        "MAPE_r = [6.73,3.63,2.99,3.49,2.93]\n",
        "r2_r = [0.76,0.91,0.94,0.93,0.94]\n",
        "accuracy_r = [93.26,96.36,97.00,96.50,97.06]\n",
        "\n",
        "# Create dataframe from the lists\n",
        "data = {'Models': models,\n",
        "        'MAE': MAE_r,\n",
        "        'MSE': MSE_r,\n",
        "        'RMSE': RMSE_r,\n",
        "        'MAPE': MAPE_r,\n",
        "        'R2': r2_r,\n",
        "        'Accuracy': accuracy_r\n",
        "       }\n",
        "metric_df = pd.DataFrame(data)\n",
        "\n",
        "# Printing dataframe\n",
        "metric_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      },
      "source": [
        "We have chosen XGboost as our final prediction model with hyperparameters {'learning_rate': 0.1, 'max_depth': 13} as it is very clear from above dataframe that it has given the highest accuracy (97%), least MAPE (3%) and maximum r2 score(0.94) on the testing dataset among all other models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvGl1hHyA_VK"
      },
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnvVTiIxBL-C"
      },
      "source": [
        "XGBoost (eXtreme Gradient Boosting) provides an efficient implementation of the gradient boosting framework. It is designed for both linear and tree-based models, and it is useful for large datasets. The basic idea behind XGBoost is to train a sequence of simple models, such as decision trees, and combine their predictions to create a more powerful model. Each tree is trained to correct the errors made by the previous trees in the sequence and this known as boosting.\n",
        "\n",
        "XGBoost uses a technique called gradient boosting to optimize the parameters of the trees. It minimizes the loss function by adjusting the parameters of the trees in a way that reduces the error of the overall model. XGBoost also includes a number of other features, such as regularization, which helps to prevent overfitting, and parallel processing, which allows for faster training times.\n",
        "\n",
        "Although tree based algorithm gives most accurate results but they have less explanability. With the help of some explanabilty tools like LIME and SHAP we can explain our model to the stakeholders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5juINNIotjk"
      },
      "source": [
        "#### Model Explainablity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETZ1_AkXoxx9"
      },
      "source": [
        "We can approach Model explainablity by two methods\n",
        "1. Globally - how features in the data collectively affect the result. eg. **Linear regression**.\n",
        "\n",
        "2. Locally- It tells us how features individually affect results eg. **Shap, LIME**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ11liGvo5CG"
      },
      "source": [
        "##### Global Explainability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bK3aiQ9kmHOh"
      },
      "outputs": [],
      "source": [
        "# Plotting the barplot to determine which feature is contributing the most\n",
        "features = final_df.columns\n",
        "importances = xgbR.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "plt.figure(figsize=(8,10))\n",
        "plt.grid(zorder=0)\n",
        "plt.title('Feature Importances', fontsize=20)\n",
        "plt.barh(range(len(indices)), importances[indices], align='center')\n",
        "plt.yticks(range(len(indices)), features[indices])\n",
        "plt.xlabel('Relative Importance')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SUo5omHmKZm"
      },
      "outputs": [],
      "source": [
        "# Checking the percentage of feature importance\n",
        "feature_imp = pd.DataFrame(columns = ['Variable','Importance'])\n",
        "feature_imp.Variable = features[:-1]\n",
        "feature_imp.Importance = importances*100\n",
        "feature_imp.sort_values(by=\"Importance\",axis=0,ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3ZJBsBapF73"
      },
      "source": [
        "As we have considered XGboost as our final optimal model with very good accuracies but still this model is considered as black box model since we don't know actually what is happening inside the algorithm. In order to gain the trust of stakeholders we have to explain the model and under which conditions the model is predicting that particular result with a valid and senseful reason. So, in order to increase the explainability we have plotted the bar plot for decresing sequence of feature importance.\n",
        "\n",
        "From the above plot it is clear that for XGboost model \"StoreType_d\" is contributing maximum i.e 34% in the final outcome, \"Customers\" is contributing 24% and followed by \"StoreType_b\", \"Promo\" and \"Promo2\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98QBgaEMpJ4U"
      },
      "source": [
        "We have achieved accuracy of 85% with the help of linear regression but to attain more accuracy and eventually scale up your business we have done this with the help of random forest, Xgboost, decision tree but they are black box models(cant explain) so we use **MODEL EXPLAINABLITY** tool **SHAP**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xySxr5WpOed"
      },
      "source": [
        "##### Explanability using SHAP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Xrz3u2EpR1T"
      },
      "source": [
        "SHAP (**Shapley Additive exPlanations**) It is used to calculate the impact of each feature of the model on the final result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daGKYZI4pYKC"
      },
      "source": [
        "Here we are using TreeExplainer (for the analysis of decision trees_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBSE7djkmmNx"
      },
      "outputs": [],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVmok3-5phzL"
      },
      "source": [
        "Explaining decision tree with **ForcePlot**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VTkeQKeplud"
      },
      "outputs": [],
      "source": [
        "# Storing our features into new variable\n",
        "feature = final_df.columns[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGtIKapeppGq"
      },
      "outputs": [],
      "source": [
        "#checking the values in the list\n",
        "feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKUhqpGepr3C"
      },
      "outputs": [],
      "source": [
        "# Checking the first observation\n",
        "x_test[0:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51xCOXSepvgq"
      },
      "outputs": [],
      "source": [
        "import shap\n",
        "#create an explainer for a tree-based model, i.e., random forest (rf)\n",
        "explainer = shap.TreeExplainer(xgbR)\n",
        "shap_values = explainer.shap_values(x_test[0:1])  #pass the first test sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2WuUjzSpz9i"
      },
      "outputs": [],
      "source": [
        "shap.initjs() #initialize the JavaScript visualization in the notebook environment\n",
        "shap.force_plot(explainer.expected_value, shap_values=shap_values[0], features = feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7E9vXAgp3TL"
      },
      "outputs": [],
      "source": [
        "y_test_rand_xgbR_pred[0:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTITUrtGp6e6"
      },
      "outputs": [],
      "source": [
        "y_test[0:1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mIlqg0mwp9d0"
      },
      "outputs": [],
      "source": [
        "x_test[0:1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7i4tJI_xqB1f"
      },
      "source": [
        "Now the first value in the index is **DayOfWeek** so we will explain our model prediction on the basis of this.\n",
        "1. We can see that the base line value is **80.22** and our function is predicting **91.07** which is quite good.\n",
        "2. **customers** and **storeType_d** are mainly responsible for that Right shift And From the opposite direction Promo and Day ofWeek acts on it to partially balance it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li9ScUyNqJnU"
      },
      "outputs": [],
      "source": [
        "#pass the second test sample\n",
        "shap_values = explainer.shap_values(x_test[1:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDERTPIdqMTi"
      },
      "outputs": [],
      "source": [
        "#initialize the JavaScript visualization in the notebook environment\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, shap_values=shap_values[0], features = feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpchURfqqQi-"
      },
      "source": [
        "Now we are taking the second index value to justify our model which is **Customers**, How far average prediction(**Base value**) away from prediction done by customer feature. The Base value is **80.22** and the predicted value is around **73.15**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mS5-QcyGqTwW"
      },
      "outputs": [],
      "source": [
        "#pass the third test sample\n",
        "shap_values = explainer.shap_values(x_test[2:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jPJvUhZqqW0g"
      },
      "outputs": [],
      "source": [
        "#initialize the JavaScript visualization in the notebook environment\n",
        "shap.initjs()\n",
        "shap.force_plot(explainer.expected_value, shap_values=shap_values[0], features = feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzsQ7noCqcfF"
      },
      "source": [
        "### Understanding the project importance for the stakeholders involved"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liU_mQIlqgor"
      },
      "source": [
        "To compete in a rapidly changing business landscape, retail stores are under increasing pressure to **anticipate market trends** and **customer behavior**. This exact demand is what our analysis is for. Our project involves various stakeholders such as Rossman Store, Customers, Inventory Managers, Society at large etc.\n",
        "\n",
        "The insights from our project highlights how the **incorporation of appropriate machine learning models( in our case XGBoost)** into their data analytics, Rossman Store gains far more accurate and powerful capabilities for **forecasting demand**, which translates into more **effective inventory management** and **big cost savings**.\n",
        "\n",
        "One of the most important segment for any retail business is **Customer Retention**. Rossman store can rely on our machine learning models to identify customers who may be ready to shop at their stores repeatedly.\n",
        "\n",
        "Another important insight from our project is **Sentiment Analysis**. Promo2, PromoIntervals etc and our analysis around them can be used to discover how people feel about their brand or product."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNgTHvd2WFk"
      },
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70Wszi-7qp06"
      },
      "source": [
        "To store our final model(object) such that we can use that in future we will use pikle module. Pickle module implements binary protocols for serializing and de-serializing a Python object structure. “Pickling” is the process where a Python object hierarchy is converted into a byte stream, and “unpickling” is the inverse operation, where a byte stream (from a binary file or bytes-like object) is converted back into an object hierarchy. Pickling (and unpickling) is alternatively known as “serialization”, “marshalling,” or “flattening”; however, to avoid confusion, the terms used here are “pickling” and “unpickling”.\n",
        "\n",
        "Pickle in Python is primarily used in serializing and deserializing a Python object structure. In other words, it's the process of converting a Python object into a byte stream to store it in a file/database, maintain program state across sessions, or transport data over the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH5McJBi2d8v"
      },
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "outputs": [],
      "source": [
        "# Importing pickle module\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XjeYjkObqwwD"
      },
      "outputs": [],
      "source": [
        "# Save the File\n",
        "filename='Rossmann_regression.pkl'\n",
        "\n",
        "# serialize process (wb=write byte)\n",
        "pickle.dump(xgb_Rand_R,open(filename,'wb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      },
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4LUjPoqnfNF"
      },
      "outputs": [],
      "source": [
        "# Load the File and predict unseen data.\n",
        "\n",
        "# unserialize process (rb=read byte)\n",
        "Regression_model= pickle.load(open(filename,'rb'))\n",
        "\n",
        "# Predicting the unseen data(test set)\n",
        "Regression_model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzGL7SERq4YO"
      },
      "outputs": [],
      "source": [
        "# Checking if we are getting the same predicted values\n",
        "y_test_rand_xgbR_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NGn_-Diq-oz"
      },
      "source": [
        "Great, we have saved our model as pickle file and can be used in future after deserialization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kee-DAl2viO"
      },
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCX9965dhzqZ"
      },
      "source": [
        "# **Conclusion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      },
      "source": [
        "Exploratory Data Analysis (EDA) is an important step because it allows for the initial investigation of a dataset. It helps to identify patterns, anomalies, and relationships in the data, as well as to detect any potential issues such as missing values or outliers. EDA also helps to generate hypotheses and inform the development of more advanced modeling techniques, such as machine learning. Additionally, it is a good way to understand the data, which is crucial for good decision making. EDA also helps to provide a deeper understanding of the data and helps to guide the direction of further analysis. After performing EDA we have drawn the following conclusions:\n",
        "\n",
        "* Sales vs Customers graph shows positive correlation between 'Sales' and\n",
        "'Customers'. As the number of customers increases, the sales also tend to increase.\n",
        "* December being a festive month attracts more sale than the rest of the months. Also, November has slightly more sales than other months. This could be due to the 'Black Friday' sale which is very popular across the globe. As Rossmann Stores deals in health and beauty products, it can be guessed that November and December sales are due to the celebratory nature of people who love to buy beauty/health products leading to the sudden increase in sales.\n",
        "* Day 1 and day 7 witness the highest sale indicating they are probably days falling on the weekend. Day 2 to day 6 generate medium to low sales indicating they are probably weekdays where customer footfall is low.\n",
        "* As stores are getting promoted, more sales are getting generated.\n",
        "* Plot between StateHolidays and sales shows that during Easter and Christmas holiday sales are actually high but for other holiday such as public ,sales are comparatively low.\n",
        "* Sales for the store type b is the highest . Store type B might be located in a more affluent or high-traffic area, which would increase the number of potential customers. Store type B may have a more favorable layout, which makes it more attractive to customers and makes it easier for them to find the products they want, resulting in more sales.\n",
        "* Sales are highest for the assortment b . This assortment may have a good mix of products that are in high demand or that are unique to the store, which would result in more sales.\n",
        "* We observed that mostly the competitor stores weren't that far from each other and the stores densely located near each other saw more sales.\n",
        "* We can conclude that Sales are high during the year 1900, as there are very few store were operated of Rossmann so there is less competition and sales are high. But as year pass on number of stores increased that means competition also increased and this leads to decline in the sales.\n",
        "* From plot sales and competition Open Since Month shows sales go increasing from Novemmber and highest in month December. This may be due to Christmas eve and New Year.\n",
        "* We can conclude from sales vs promo2 graph that customers are slightly less responsive to the stores(i.e sales) that are running consecutive promotions. One possibility could be customers might have already taken advantage of a similar promotion earlier. Another reason could be store might not have invested enough in promoting the promotion to customers, resulting in lower awareness and fewer sales. Also, if the store is running same promotion again and again, it could have resulted into lower customer footfal and ultimately leadind to fewer sales.\n",
        "* Sales vs Promo2SinceYear barplot explains that sales were still the highest when the store wasn't running any consecutive promotional events. But in 2014, the sales were really shoot up and they are recorded as 2nd highest. Good quality products, better deals, shutdown of competitions etc could be the reasons.\n",
        "* We can see the promo interval Jan, Apr, Jul, Oct records the 2nd highest sales as it marks the festive season. However, the other intervals are recording sales that are close to the 1st interval."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMyfPrswrP_z"
      },
      "source": [
        "##**Conclusions drawn from ML Model Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIW1PE-_rWSq"
      },
      "source": [
        "Close predictions of any ML model highly impacts the business growth. Before going to further model deployment one should have to check how accurately the model is predicting and performing with the real life data.\n",
        "\n",
        "Conclusions drawn from any model are very helpful to identify wheather the model is fully baked and good to go for deployment process or needs further refinement. In this section first we will talk about some general points that are essential for every ML model and then will talk about the project oriented conclusions we made:\n",
        "\n",
        "**General conclusions**:\n",
        "\n",
        "\n",
        "*   The implementation of an ML model can greatly\n",
        "improve the performance and accuracy of a system or application.\n",
        "*   It is important to carefully select and preprocess the data used for training and testing the model.\n",
        "*   Regular evaluation and tuning of the model is necessary to ensure optimal performance.\n",
        "*   The use of appropriate evaluation metrics can help to measure the performance of the model.\n",
        "*   The integration of the model into the system or application should be done in a way that allows for easy maintenance and updates.\n",
        "*   The ethical and legal considerations of the model's use should also be taken into account.\n",
        "\n",
        "**Project conclusions**:\n",
        "\n",
        "We have implemented various regression model started with Linear Regression and then we have tried other non linear models too. For each of the model we have tried to tune the hyperparameters as well in order to minimize the errors and drawn following conclusions\n",
        "*   In Linear Regression we got the accuracy of ~93% and the model is capturing 75% of variance even after using regularization techniques that means our data is not perfectly  linearly dependent with target variable(Sales).\n",
        "*   For Decsion Tree we have achieved ~96.3% accuracy with maximum depth of 18 and on increasing the depth over it we are falling towards overfitting and MAPE of 3.6% which ultimately increases the mean absolute percentage error.\n",
        "*   Giving preference to each of the variable always results in better accuracy as small subsets can provide significant accuracy percentage. Ensemble technique i.e Random Forest has given the accuray of ~96.96% with total trees of 100 in the forest with hyperparameter tuning.\n",
        "*   We have also tried gradiant boosting technique with LightGBM although we got the similar results as Random Forest(~96.4%) but we got the more fast results as it has used all the cores and decrease processing time. While training the large dataset one should try LightGBM for good results in less time.\n",
        "*   At last we have implemented our final model i.e XGboost and achieved the accuracy of 97% with mean absolute percentage error of only 2%. Also we got the mean and median of residuals at 0.09 which is indicating towards excellent residual plot.\n",
        "\n",
        "Since, a good residual plot have the following characteristics:\n",
        "> 1. It should contain high density of points close to the origin and a low density of points away from the origin.\n",
        "> 2. It is symmetric about the origin.\n",
        "\n",
        "The residual plot obtained from XGboost of fulfilling both the characteristics and from the above experiments and identifications we have choosen the XGboost as our final optimal model for deployment."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "cell_execution_strategy": "setup",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}